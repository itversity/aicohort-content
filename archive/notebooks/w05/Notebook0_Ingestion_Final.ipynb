{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 0: Data Ingestion for Toyota Specifications\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This notebook ingests Toyota specification PDFs and creates **hybrid chunks** that support:\n",
        "\n",
        "1. **Semantic Search**: Natural language queries like \"comfortable family sedan\"\n",
        "2. **Metadata Filtering**: Precise filters like `{\"model\": \"Camry\", \"mpg_combined\": {\">\":30}}`\n",
        "\n",
        "## Why Hybrid Chunks?\n",
        "\n",
        "Notebook1's planning concepts require BOTH capabilities:\n",
        "- **Multi-query variants** need semantic text (\"spacious\", \"reliable\", \"efficient\")\n",
        "- **Subquery filters** need structured KeySpec metadata\n",
        "\n",
        "## Strategy\n",
        "\n",
        "Each chunk contains:\n",
        "- **Full PDF text** (~2,843 chars avg) for semantic search\n",
        "- **Structured specifications** for readability  \n",
        "- **KeySpec metadata** for precise filtering\n",
        "\n",
        "Total chunk size: ~3,500 chars (~875 tokens) - well within text-embedding-005 limits!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Package Installation\n",
        "\n",
        "In my case I have installed all the dependencies already following these instructions:\n",
        "\n",
        "```shell\n",
        "# Create Python virtual environment\n",
        "python3.11 -m venv .venv\n",
        "\n",
        "# Activate newly created Python virtual environment\n",
        "source .venv/bin/activate\n",
        "\n",
        "# Install all the dependencies using `requirements.txt`\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip -q install google-cloud-aiplatform vertexai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip -q install chromadb pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vertexai 1.71.1 requires google-cloud-aiplatform[all]==1.71.1, but you have google-cloud-aiplatform 1.126.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip -q install langchain-community langchain-google-vertexai langchain-core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Environment Configuration\n",
        "\n",
        "Configuration matches Notebook1 exactly to ensure compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = \"agentapps-473813\"\n",
        "REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  DATA_DIR: ../data/car-specs/toyota-specs\n",
            "  PERSIST_DIR: ../data/chroma\n",
            "  COLLECTION: toyota_specs\n",
            "  EMBED_MODEL: text-embedding-005\n",
            "  LLM_MODEL: meta/llama-3.3-70b-instruct-maas\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = \"../data/car-specs/toyota-specs\"\n",
        "PERSIST_DIR = \"../data/chroma\"\n",
        "COLLECTION_NAME = \"toyota_specs\"\n",
        "\n",
        "# Models\n",
        "EMBED_MODEL_ID = \"text-embedding-005\"\n",
        "EMBED_OUTPUT_DIM = 256\n",
        "LLAMA_MODEL_ID = \"meta/llama-3.3-70b-instruct-maas\"\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  DATA_DIR: {DATA_DIR}\")\n",
        "print(f\"  PERSIST_DIR: {PERSIST_DIR}\")\n",
        "print(f\"  COLLECTION: {COLLECTION_NAME}\")\n",
        "print(f\"  EMBED_MODEL: {EMBED_MODEL_ID}\")\n",
        "print(f\"  LLM_MODEL: {LLAMA_MODEL_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Initialize Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Vertex AI initialized\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/itversity/Projects/Internal/aicohort-content/.venv/lib/python3.11/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "print(\"âœ“ Vertex AI initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Schema Definition (KeySpec)\n",
        "\n",
        "### Critical: Metadata Alignment\n",
        "\n",
        "This `KeySpec` schema **MUST match Notebook1 exactly** (lines 434-446).\n",
        "\n",
        "**Why?**\n",
        "- Notebook1's planning generates filters like `{\"mpg_combined\": {\">\":30}}`\n",
        "- These filters will fail if field names don't match!\n",
        "\n",
        "### Field Support Matrix\n",
        "\n",
        "| Field | Semantic Search | Metadata Filter | Example Query |\n",
        "|-------|----------------|-----------------|---------------|\n",
        "| model | âœ… (via text) | âœ… | \"Show me Camry options\" |\n",
        "| trim | âœ… (via text) | âœ… | \"TRD Pro trim\" |\n",
        "| mpg_combined | âŒ | âœ… | \"MPG > 30\" |\n",
        "| ev_only_range_mi | âŒ | âœ… | \"EV range > 40 miles\" |\n",
        "| towing_max_lbs | âŒ | âœ… | \"Can tow 6000 lbs\" |\n",
        "| seats | âŒ | âœ… | \"Seats >= 7\" |\n",
        "| drivetrain | âœ… (via text) | âœ… | \"AWD vehicles\" |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ KeySpec schema defined\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Optional, Dict, Any\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class KeySpec(BaseModel):\n",
        "    \"\"\"\n",
        "    Canonical metadata schema for Toyota specs.\n",
        "    MUST match Notebook1's KeySpec (lines 434-446) exactly!\n",
        "    \"\"\"\n",
        "    model: str  # REQUIRED: e.g., \"Camry\", \"Prius\", \"RAV4\"\n",
        "    trim: Optional[str] = None  # e.g., \"LE\", \"XLE\", \"TRD Pro\"\n",
        "    mpg_city: Optional[float] = None\n",
        "    mpg_hwy: Optional[float] = None\n",
        "    mpg_combined: Optional[float] = None\n",
        "    ev_only_range_mi: Optional[float] = None\n",
        "    total_range_mi: Optional[float] = None\n",
        "    towing_max_lbs: Optional[float] = None\n",
        "    seats: Optional[int] = None\n",
        "    drivetrain: Optional[str] = None  # e.g., \"AWD\", \"FWD\", \"4WD\"\n",
        "    starting_price_mentions: Optional[List[str]] = None\n",
        "\n",
        "print(\"âœ“ KeySpec schema defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: PDF Extraction Functions\n",
        "\n",
        "### PDF Size Analysis\n",
        "\n",
        "Our PDFs are small:\n",
        "- **Average**: 2,843 characters (~711 tokens)\n",
        "- **Largest**: 3,310 characters (Prius)\n",
        "- **Embedding limit**: ~3,072 tokens (~12,000 chars)\n",
        "\n",
        "**Conclusion**: We can safely include full PDF text in each chunk!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ PDF extraction functions defined\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def list_pdf_files(data_dir: str) -> List[Path]:\n",
        "    \"\"\"List all PDF files in directory.\"\"\"\n",
        "    p = Path(data_dir)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Directory not found: {data_dir}\")\n",
        "    files = sorted(list(p.glob(\"*.pdf\")))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No PDFs found in {data_dir}\")\n",
        "    return files\n",
        "\n",
        "def extract_text_from_pdf(path: Path) -> str:\n",
        "    \"\"\"\n",
        "    Extract full text from PDF.\n",
        "    NO truncation - our PDFs are small enough!\n",
        "    \"\"\"\n",
        "    reader = PdfReader(str(path))\n",
        "    chunks = []\n",
        "    for pg in reader.pages:\n",
        "        try:\n",
        "            text = pg.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            text = \"\"\n",
        "        if text:\n",
        "            chunks.append(text)\n",
        "    return \"\\n\".join(chunks)\n",
        "\n",
        "print(\"âœ“ PDF extraction functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8 PDFs:\n",
            "  - Introduction_to_Toyota_Car_Sales.pdf\n",
            "  - Toyota_Camry_Specifications.pdf\n",
            "  - Toyota_Corolla_Specifications.pdf\n",
            "  - Toyota_Highlander_Specifications.pdf\n",
            "  - Toyota_Prius_Specifications.pdf\n",
            "  - Toyota_RAV4_Specifications.pdf\n",
            "  - Toyota_Tacoma_Specifications.pdf\n",
            "  - Toyota_bZ4X_Specifications.pdf\n"
          ]
        }
      ],
      "source": [
        "# Verify PDFs exist\n",
        "pdf_files = list_pdf_files(DATA_DIR)\n",
        "print(f\"Found {len(pdf_files)} PDFs:\")\n",
        "for pdf in pdf_files:\n",
        "    print(f\"  - {pdf.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: LLM Setup for Spec Extraction\n",
        "\n",
        "We use the same Llama model as Notebook1 to extract structured KeySpec data from PDFs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ LLM initialized: meta/llama-3.3-70b-instruct-maas\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_vertexai.model_garden_maas.llama import VertexModelGardenLlama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Initialize LLM\n",
        "chat_llm = VertexModelGardenLlama(\n",
        "    model=LLAMA_MODEL_ID,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "print(f\"âœ“ LLM initialized: {LLAMA_MODEL_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Extraction prompt configured\n"
          ]
        }
      ],
      "source": [
        "# Setup extraction chain\n",
        "extraction_parser = JsonOutputParser(pydantic_object=KeySpec)\n",
        "\n",
        "extraction_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a strict JSON generator. Return ONLY valid JSON.\"),\n",
        "    (\"human\", \"\"\"Extract ALL model/trim configurations from this Toyota PDF.\n",
        "Return a JSON array of objects matching this schema: {schema}\n",
        "\n",
        "Extract:\n",
        "- model (required): e.g., \"Camry\", \"Prius\", \"RAV4\"\n",
        "- trim: e.g., \"LE\", \"XLE\", \"TRD Pro\", \"Hybrid\"\n",
        "- mpg_city, mpg_hwy, mpg_combined (if mentioned)\n",
        "- ev_only_range_mi, total_range_mi (for hybrids/EVs)\n",
        "- towing_max_lbs (if mentioned)\n",
        "- seats (if mentioned)\n",
        "- drivetrain: \"FWD\", \"AWD\", or \"4WD\"\n",
        "- starting_price_mentions: list of price strings mentioned\n",
        "\n",
        "PDF: {filename}\n",
        "TEXT:\n",
        "{doc_text}\n",
        "\"\"\"),\n",
        "])\n",
        "\n",
        "print(\"âœ“ Extraction prompt configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Spec Extraction Function\n",
        "\n",
        "This function extracts structured KeySpec data AND returns the original text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Extraction function defined\n"
          ]
        }
      ],
      "source": [
        "def extract_specs_from_pdf_llm(pdf_path: Path, llm) -> tuple[List[KeySpec], str]:\n",
        "    \"\"\"\n",
        "    Extract structured specs AND original text from PDF.\n",
        "    \n",
        "    Returns:\n",
        "        (specs_list, original_text)\n",
        "        - specs_list: List of KeySpec objects (one per trim)\n",
        "        - original_text: Full PDF text for semantic search\n",
        "    \"\"\"\n",
        "    # Extract full text\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    \n",
        "    # Call LLM to extract structured data\n",
        "    chain = extraction_prompt | llm | extraction_parser\n",
        "    result = chain.invoke({\n",
        "        \"schema\": KeySpec.model_json_schema(),\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"doc_text\": text,\n",
        "    })\n",
        "    \n",
        "    # Normalize to list\n",
        "    if isinstance(result, dict):\n",
        "        result = [result]\n",
        "    \n",
        "    # Convert to KeySpec objects\n",
        "    specs = [KeySpec.model_validate(item) for item in result]\n",
        "    \n",
        "    # Return BOTH specs and original text\n",
        "    return specs, text\n",
        "\n",
        "print(\"âœ“ Extraction function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Hybrid Chunking Function\n",
        "\n",
        "### Why Hybrid Chunks?\n",
        "\n",
        "**Problem with metadata-only chunks:**\n",
        "```\n",
        "Content: \"Model: Camry\\nTrim: LE\\nDrivetrain: FWD\"\n",
        "```\n",
        "âŒ Won't match \"comfortable family sedan\"  \n",
        "âŒ Won't match \"reliable car for professionals\"  \n",
        "\n",
        "**Solution: Full PDF text + structured specs:**\n",
        "```\n",
        "Content: \"Toyota Camry is a premium midsize sedan renowned for its \n",
        "reliability, spacious interiors, and smooth performance. It caters\n",
        "to professionals, small families...\n",
        "\n",
        "STRUCTURED SPECIFICATIONS\n",
        "Model: Camry\n",
        "Trim: LE\n",
        "...\"\n",
        "```\n",
        "âœ… Matches semantic queries  \n",
        "âœ… Has structured metadata for filtering  \n",
        "âœ… Supports Notebook1's multi-query variants  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Hybrid chunking function defined\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "def create_chunks_from_specs(\n",
        "    pdf_path: Path,\n",
        "    specs: List[KeySpec],\n",
        "    original_text: str\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Create hybrid chunks: Full PDF text + structured specs.\n",
        "    One chunk per model/trim configuration.\n",
        "    \n",
        "    PDFs average 2,843 chars - well within embedding limits!\n",
        "    Final chunks: ~3,500 chars (PDF + specs) = ~875 tokens.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    \n",
        "    for i, spec in enumerate(specs):\n",
        "        # Build structured specs section\n",
        "        spec_lines = [f\"Model: {spec.model}\"]\n",
        "        \n",
        "        if spec.trim:\n",
        "            spec_lines.append(f\"Trim: {spec.trim}\")\n",
        "        \n",
        "        if spec.mpg_combined:\n",
        "            spec_lines.append(f\"Fuel Economy: {spec.mpg_combined} MPG combined\")\n",
        "            if spec.mpg_city and spec.mpg_hwy:\n",
        "                spec_lines.append(f\"  ({spec.mpg_city} city / {spec.mpg_hwy} highway)\")        \n",
        "        if spec.ev_only_range_mi:\n",
        "            spec_lines.append(f\"EV-Only Range: {spec.ev_only_range_mi} miles\")\n",
        "        \n",
        "        if spec.total_range_mi:\n",
        "            spec_lines.append(f\"Total Range: {spec.total_range_mi} miles\")\n",
        "        \n",
        "        if spec.towing_max_lbs:\n",
        "            spec_lines.append(f\"Towing Capacity: {spec.towing_max_lbs} lbs\")\n",
        "        \n",
        "        if spec.seats:\n",
        "            spec_lines.append(f\"Seating: {spec.seats} passengers\")\n",
        "        \n",
        "        if spec.drivetrain:\n",
        "            spec_lines.append(f\"Drivetrain: {spec.drivetrain}\")\n",
        "        \n",
        "        if spec.starting_price_mentions:\n",
        "            spec_lines.append(f\"Pricing: {', '.join(spec.starting_price_mentions)}\")\n",
        "        \n",
        "        # HYBRID CONTENT: Full PDF text + structured specs\n",
        "        page_content = f\"\"\"{original_text}\n",
        "\n",
        "{'='*60}\n",
        "STRUCTURED SPECIFICATIONS\n",
        "{'='*60}\n",
        "\n",
        "{chr(10).join(spec_lines)}\"\"\"        \n",
        "        # Metadata: All KeySpec fields (ChromaDB compatible)\n",
        "        metadata = {\n",
        "            \"source\": pdf_path.name,\n",
        "            \"chunk_id\": f\"{pdf_path.stem}_{i:03d}\",\n",
        "            \"model\": spec.model,\n",
        "        }\n",
        "        \n",
        "        # Add optional fields\n",
        "        if spec.trim:\n",
        "            metadata[\"trim\"] = spec.trim\n",
        "        if spec.mpg_city:\n",
        "            metadata[\"mpg_city\"] = spec.mpg_city\n",
        "        if spec.mpg_hwy:\n",
        "            metadata[\"mpg_hwy\"] = spec.mpg_hwy\n",
        "        if spec.mpg_combined:\n",
        "            metadata[\"mpg_combined\"] = spec.mpg_combined\n",
        "        if spec.ev_only_range_mi:\n",
        "            metadata[\"ev_only_range_mi\"] = spec.ev_only_range_mi\n",
        "        if spec.total_range_mi:\n",
        "            metadata[\"total_range_mi\"] = spec.total_range_mi\n",
        "        if spec.towing_max_lbs:\n",
        "            metadata[\"towing_max_lbs\"] = spec.towing_max_lbs\n",
        "        if spec.seats:\n",
        "            metadata[\"seats\"] = spec.seats\n",
        "        if spec.drivetrain:\n",
        "            metadata[\"drivetrain\"] = spec.drivetrain\n",
        "        \n",
        "        # Convert list to string for ChromaDB\n",
        "        if spec.starting_price_mentions:\n",
        "            metadata[\"starting_price_mentions\"] = \", \".join(spec.starting_price_mentions)\n",
        "        \n",
        "        chunks.append(Document(\n",
        "            page_content=page_content,\n",
        "            metadata=metadata\n",
        "        ))\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "print(\"âœ“ Hybrid chunking function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: ChromaDB Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Embeddings model ready: text-embedding-005\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "# Initialize embeddings model\n",
        "embeddings_model = VertexAIEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "print(f\"âœ“ Embeddings model ready: {EMBED_MODEL_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/n4/66bq258d6xq6lscwjlgs6q500000gn/T/ipykernel_104/4248553841.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Vectorstore ready: toyota_specs\n",
            "  Persist directory: ../data/chroma\n",
            "  Current documents: 0\n"
          ]
        }
      ],
      "source": [
        "# Initialize vectorstore\n",
        "vectorstore = Chroma(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    persist_directory=PERSIST_DIR,\n",
        "    embedding_function=embeddings_model\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Vectorstore ready: {COLLECTION_NAME}\")\n",
        "print(f\"  Persist directory: {PERSIST_DIR}\")\n",
        "\n",
        "# Check current size\n",
        "try:\n",
        "    current_count = vectorstore._collection.count()\n",
        "    print(f\"  Current documents: {current_count}\")\n",
        "except:\n",
        "    print(f\"  Collection is empty or new\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Batched Ingestion Pipeline\n",
        "\n",
        "### Why Batch Processing?\n",
        "\n",
        "- **Avoids rate limits**: Process 5 chunks, wait 2s, repeat\n",
        "- **Better error handling**: Can retry individual batches\n",
        "- **Progress tracking**: Shows which chunks are being embedded\n",
        "\n",
        "### force_rebuild Parameter\n",
        "\n",
        "- `True`: Clears all 123 old documents, rebuilds from scratch\n",
        "- `False`: Adds to existing collection (may cause duplicates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Batched ingestion function defined\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def ingest_toyota_pdfs_batched(\n",
        "    pdf_dir: str,\n",
        "    vectorstore: Chroma,\n",
        "    llm,\n",
        "    force_rebuild: bool = False,\n",
        "    batch_size: int = 5,\n",
        "    delay_between_batches: float = 2.0\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Batch ingestion with delays to avoid rate limits.\n",
        "    \n",
        "    Args:\n",
        "        batch_size: Process N chunks, embed, then delay\n",
        "        delay_between_batches: Seconds to wait between batches\n",
        "    \"\"\"\n",
        "    if force_rebuild:\n",
        "        print(\"ðŸ”„ Force rebuild: Clearing collection...\")\n",
        "        vectorstore.delete_collection()\n",
        "        vectorstore = Chroma(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            persist_directory=PERSIST_DIR,\n",
        "            embedding_function=embeddings_model\n",
        "        )\n",
        "        print(\"   âœ“ Collection cleared\")\n",
        "    \n",
        "    pdf_files = list_pdf_files(pdf_dir)\n",
        "    print(f\"\\nðŸ“„ Found {len(pdf_files)} PDFs\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    all_chunks = []\n",
        "    stats = {\n",
        "        \"total_pdfs\": len(pdf_files),\n",
        "        \"total_chunks\": 0,\n",
        "        \"pdfs_processed\": [],\n",
        "        \"errors\": []\n",
        "    }\n",
        "    \n",
        "    for pdf_path in pdf_files:\n",
        "        try:\n",
        "            print(f\"\\nðŸ“– Processing: {pdf_path.name}\")\n",
        "            \n",
        "            # Extract specs AND original text\n",
        "            specs, original_text = extract_specs_from_pdf_llm(pdf_path, llm)\n",
        "            print(f\"  âœ“ Extracted {len(specs)} configurations\")\n",
        "            \n",
        "            # Create hybrid chunks\n",
        "            chunks = create_chunks_from_specs(pdf_path, specs, original_text)\n",
        "            print(f\"  âœ“ Created {len(chunks)} chunks\")\n",
        "            \n",
        "            all_chunks.extend(chunks)\n",
        "            stats[\"pdfs_processed\"].append({\n",
        "                \"filename\": pdf_path.name,\n",
        "                \"configs\": len(specs),\n",
        "                \"chunks\": len(chunks)\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âœ— Error: {e}\")\n",
        "            stats[\"errors\"].append({\n",
        "                \"file\": pdf_path.name,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "    \n",
        "    # Add chunks in batches with delays\n",
        "    if all_chunks:\n",
        "        print(f\"\\nðŸ’¾ Adding {len(all_chunks)} chunks in batches...\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        for i in range(0, len(all_chunks), batch_size):\n",
        "            batch = all_chunks[i:i+batch_size]\n",
        "            batch_num = i//batch_size + 1\n",
        "            total_batches = (len(all_chunks) + batch_size - 1) // batch_size\n",
        "            \n",
        "            print(f\"\\n  Batch {batch_num}/{total_batches}: Adding {len(batch)} chunks...\")\n",
        "            vectorstore.add_documents(batch)\n",
        "            print(f\"  âœ“ Batch {batch_num} embedded\")\n",
        "            \n",
        "            if i + batch_size < len(all_chunks):\n",
        "                print(f\"  â³ Waiting {delay_between_batches}s before next batch...\")\n",
        "                time.sleep(delay_between_batches)\n",
        "        \n",
        "        print(f\"\\nâœ… All {len(all_chunks)} chunks successfully added!\")\n",
        "    \n",
        "    stats[\"total_chunks\"] = len(all_chunks)\n",
        "    return stats\n",
        "\n",
        "print(\"âœ“ Batched ingestion function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 11: Run Ingestion\n",
        "\n",
        "**âš ï¸ Set `force_rebuild=True` to clear old data!**\n",
        "\n",
        "This will:\n",
        "1. Delete 123 old documents\n",
        "2. Process 8 PDFs\n",
        "3. Extract ~29 configurations\n",
        "4. Create 29 hybrid chunks\n",
        "5. Embed in batches of 5 with 2s delays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Force rebuild: Clearing collection...\n",
            "   âœ“ Collection cleared\n",
            "\n",
            "ðŸ“„ Found 8 PDFs\n",
            "============================================================\n",
            "\n",
            "ðŸ“– Processing: Introduction_to_Toyota_Car_Sales.pdf\n",
            "  âœ“ Extracted 0 configurations\n",
            "  âœ“ Created 0 chunks\n",
            "\n",
            "ðŸ“– Processing: Toyota_Camry_Specifications.pdf\n",
            "  âœ“ Extracted 4 configurations\n",
            "  âœ“ Created 4 chunks\n",
            "\n",
            "ðŸ“– Processing: Toyota_Corolla_Specifications.pdf\n",
            "  âœ“ Extracted 5 configurations\n",
            "  âœ“ Created 5 chunks\n",
            "\n",
            "ðŸ“– Processing: Toyota_Highlander_Specifications.pdf\n",
            "  âœ“ Extracted 7 configurations\n",
            "  âœ“ Created 7 chunks\n",
            "\n",
            "ðŸ“– Processing: Toyota_Prius_Specifications.pdf\n",
            "  âœ“ Extracted 7 configurations\n",
            "  âœ“ Created 7 chunks\n",
            "\n",
            "ðŸ“– Processing: Toyota_RAV4_Specifications.pdf\n",
            "  âœ“ Extracted 7 configurations\n",
            "  âœ“ Created 7 chunks\n",
            "\n",
            "ðŸ“– Processing: Toyota_Tacoma_Specifications.pdf\n",
            "  âœ“ Extracted 3 configurations\n",
            "  âœ“ Created 3 chunks\n",
            "\n",
            "ðŸ“– Processing: Toyota_bZ4X_Specifications.pdf\n",
            "  âœ“ Extracted 2 configurations\n",
            "  âœ“ Created 2 chunks\n",
            "\n",
            "ðŸ’¾ Adding 35 chunks in batches...\n",
            "============================================================\n",
            "\n",
            "  Batch 1/7: Adding 5 chunks...\n",
            "  âœ“ Batch 1 embedded\n",
            "  â³ Waiting 2.0s before next batch...\n",
            "\n",
            "  Batch 2/7: Adding 5 chunks...\n",
            "  âœ“ Batch 2 embedded\n",
            "  â³ Waiting 2.0s before next batch...\n",
            "\n",
            "  Batch 3/7: Adding 5 chunks...\n",
            "  âœ“ Batch 3 embedded\n",
            "  â³ Waiting 2.0s before next batch...\n",
            "\n",
            "  Batch 4/7: Adding 5 chunks...\n",
            "  âœ“ Batch 4 embedded\n",
            "  â³ Waiting 2.0s before next batch...\n",
            "\n",
            "  Batch 5/7: Adding 5 chunks...\n",
            "  âœ“ Batch 5 embedded\n",
            "  â³ Waiting 2.0s before next batch...\n",
            "\n",
            "  Batch 6/7: Adding 5 chunks...\n",
            "  âœ“ Batch 6 embedded\n",
            "  â³ Waiting 2.0s before next batch...\n",
            "\n",
            "  Batch 7/7: Adding 5 chunks...\n",
            "  âœ“ Batch 7 embedded\n",
            "\n",
            "âœ… All 35 chunks successfully added!\n",
            "\n",
            "============================================================\n",
            "INGESTION COMPLETE\n",
            "============================================================\n",
            "PDFs processed: 8\n",
            "Total chunks: 35\n",
            "Successful: 8\n",
            "\n",
            "âœ… No errors\n"
          ]
        }
      ],
      "source": [
        "ingestion_stats = ingest_toyota_pdfs_batched(\n",
        "    pdf_dir=DATA_DIR,\n",
        "    vectorstore=vectorstore,\n",
        "    llm=chat_llm,\n",
        "    force_rebuild=True,  # â¬…ï¸ SET TO TRUE TO CLEAR OLD DATA\n",
        "    batch_size=5,\n",
        "    delay_between_batches=2.0\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INGESTION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"PDFs processed: {ingestion_stats['total_pdfs']}\")\n",
        "print(f\"Total chunks: {ingestion_stats['total_chunks']}\")\n",
        "print(f\"Successful: {len(ingestion_stats['pdfs_processed'])}\")\n",
        "\n",
        "if ingestion_stats['errors']:\n",
        "    print(f\"\\nâš ï¸ Errors: {len(ingestion_stats['errors'])}\")\n",
        "    for err in ingestion_stats['errors']:\n",
        "        print(f\"  - {err['file']}: {err['error']}\")\n",
        "else:\n",
        "    print(\"\\nâœ… No errors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reconnect to New Collection\n",
        "\n",
        "**Why is this needed?**\n",
        "\n",
        "When `force_rebuild=True`, the ingestion function:\n",
        "1. Deletes the old collection\n",
        "2. Creates a NEW collection with a NEW ID\n",
        "3. But due to Python variable scoping, the global `vectorstore` variable still points to the OLD (deleted) collection\n",
        "\n",
        "We need to reinitialize `vectorstore` to point to the new collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Vectorstore reconnected to new collection\n",
            "  Documents in collection: 35\n"
          ]
        }
      ],
      "source": [
        "# âš ï¸ IMPORTANT: Reconnect to collection after force_rebuild\n",
        "# The ingestion function created a new collection with a new ID\n",
        "# We need to reinitialize the vectorstore variable to point to it\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    persist_directory=PERSIST_DIR,\n",
        "    embedding_function=embeddings_model\n",
        ")\n",
        "\n",
        "print(\"âœ“ Vectorstore reconnected to new collection\")\n",
        "try:\n",
        "    doc_count = vectorstore._collection.count()\n",
        "    print(f\"  Documents in collection: {doc_count}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not get count: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 12: Validation - Semantic Search\n",
        "\n",
        "Test that natural language queries work. These queries should match chunks based on **descriptive text**, not just metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SEMANTIC SEARCH VALIDATION\n",
            "============================================================\n",
            "\n",
            "Query: 'comfortable family sedan'\n",
            "------------------------------------------------------------\n",
            "  1. Corolla XSE\n",
            "     Content length: 2893 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Corolla_Specifications.md Toyota Corolla: A Best-Selling Sedan Overview The Toyota Corolla is one of the best-selling sedans worldwide, kno...\n",
            "  2. Corolla SE\n",
            "     Content length: 2875 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Corolla_Specifications.md Toyota Corolla: A Best-Selling Sedan Overview The Toyota Corolla is one of the best-selling sedans worldwide, kno...\n",
            "  3. Corolla LE\n",
            "     Content length: 2892 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Corolla_Specifications.md Toyota Corolla: A Best-Selling Sedan Overview The Toyota Corolla is one of the best-selling sedans worldwide, kno...\n",
            "\n",
            "Query: 'reliable car for professionals'\n",
            "------------------------------------------------------------\n",
            "  1. Corolla XSE\n",
            "     Content length: 2893 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Corolla_Specifications.md Toyota Corolla: A Best-Selling Sedan Overview The Toyota Corolla is one of the best-selling sedans worldwide, kno...\n",
            "  2. Corolla SE\n",
            "     Content length: 2875 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Corolla_Specifications.md Toyota Corolla: A Best-Selling Sedan Overview The Toyota Corolla is one of the best-selling sedans worldwide, kno...\n",
            "  3. Corolla Hybrid\n",
            "     Content length: 2896 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Corolla_Specifications.md Toyota Corolla: A Best-Selling Sedan Overview The Toyota Corolla is one of the best-selling sedans worldwide, kno...\n",
            "\n",
            "Query: 'eco-friendly hybrid vehicle'\n",
            "------------------------------------------------------------\n",
            "  1. Prius \n",
            "     Content length: 3489 chars\n",
            "     Has semantic text: âŒ\n",
            "     Preview: rag/Toyota_Prius_Specifications.md Toyota Prius & Prius Prime: Eco-Friendly Hybrids Overview The Toyota Prius is a pioneer in hybrid technology, renow...\n",
            "  2. Prius LE\n",
            "     Content length: 3515 chars\n",
            "     Has semantic text: âŒ\n",
            "     Preview: rag/Toyota_Prius_Specifications.md Toyota Prius & Prius Prime: Eco-Friendly Hybrids Overview The Toyota Prius is a pioneer in hybrid technology, renow...\n",
            "  3. Prius XLE\n",
            "     Content length: 3516 chars\n",
            "     Has semantic text: âŒ\n",
            "     Preview: rag/Toyota_Prius_Specifications.md Toyota Prius & Prius Prime: Eco-Friendly Hybrids Overview The Toyota Prius is a pioneer in hybrid technology, renow...\n",
            "\n",
            "Query: 'powerful off-road truck'\n",
            "------------------------------------------------------------\n",
            "  1. Tacoma TRD Sport\n",
            "     Content length: 2909 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Tacoma_Specifications.md Toyota Tacoma: The Reliable Midsize Pickup Truck Overview The Toyota Tacoma is a rugged and versatile midsize pick...\n",
            "  2. Tacoma TRD Pro\n",
            "     Content length: 2907 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Tacoma_Specifications.md Toyota Tacoma: The Reliable Midsize Pickup Truck Overview The Toyota Tacoma is a rugged and versatile midsize pick...\n",
            "  3. Tacoma SR\n",
            "     Content length: 2902 chars\n",
            "     Has semantic text: âœ…\n",
            "     Preview: rag/Toyota_Tacoma_Specifications.md Toyota Tacoma: The Reliable Midsize Pickup Truck Overview The Toyota Tacoma is a rugged and versatile midsize pick...\n",
            "\n",
            "============================================================\n",
            "âœ… Semantic search validation complete\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"SEMANTIC SEARCH VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "semantic_queries = [\n",
        "    \"comfortable family sedan\",\n",
        "    \"reliable car for professionals\",\n",
        "    \"eco-friendly hybrid vehicle\",\n",
        "    \"powerful off-road truck\"\n",
        "]\n",
        "\n",
        "for query in semantic_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    print(\"-\"*60)\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    \n",
        "    for i, doc in enumerate(results, 1):\n",
        "        model = doc.metadata.get('model', 'N/A')\n",
        "        trim = doc.metadata.get('trim', '')\n",
        "        text_len = len(doc.page_content)\n",
        "        \n",
        "        print(f\"  {i}. {model} {trim}\")\n",
        "        print(f\"     Content length: {text_len} chars\")\n",
        "        \n",
        "        # Verify has semantic text\n",
        "        has_semantic = any(word in doc.page_content.lower() \n",
        "                          for word in ['reliable', 'comfortable', 'efficient', 'family', 'professional'])\n",
        "        print(f\"     Has semantic text: {'âœ…' if has_semantic else 'âŒ'}\")\n",
        "        \n",
        "        if text_len > 100:\n",
        "            preview = doc.page_content[:150].replace('\\n', ' ')\n",
        "            print(f\"     Preview: {preview}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Semantic search validation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 13: Validation - Metadata Filtering\n",
        "\n",
        "Test that KeySpec metadata filters work correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "METADATA FILTERING VALIDATION\n",
            "============================================================\n",
            "\n",
            "1. Filter: model='Camry'\n",
            "   Found 4 Camry chunks\n",
            "     - Camry XSE V6\n",
            "     - Camry SE\n",
            "     - Camry Hybrid\n",
            "\n",
            "2. Filter: mpg_combined >= 30\n",
            "   Found 3 high-efficiency chunks\n",
            "     - Prius Prime SE: 133.0 MPG\n",
            "     - Prius Prime XSE: 133.0 MPG\n",
            "     - Prius Prime XSE Premium: 133.0 MPG\n",
            "\n",
            "3. Filter: model='RAV4' AND trim='TRD Pro'\n",
            "   No matches found (check if this trim exists)\n",
            "\n",
            "============================================================\n",
            "âœ… Metadata filtering validation complete\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"METADATA FILTERING VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test 1: Filter by model\n",
        "print(\"\\n1. Filter: model='Camry'\")\n",
        "results = vectorstore.similarity_search(\"Toyota\", k=10, filter={\"model\": \"Camry\"})\n",
        "print(f\"   Found {len(results)} Camry chunks\")\n",
        "if results:\n",
        "    for doc in results[:3]:\n",
        "        trim = doc.metadata.get('trim', 'N/A')\n",
        "        print(f\"     - Camry {trim}\")\n",
        "\n",
        "# Test 2: Filter by MPG\n",
        "print(\"\\n2. Filter: mpg_combined >= 30\")\n",
        "try:\n",
        "    results = vectorstore.get(where={\"mpg_combined\": {\"$gte\": 30}})\n",
        "    print(f\"   Found {len(results['ids'])} high-efficiency chunks\")\n",
        "    if results['metadatas']:\n",
        "        for meta in results['metadatas'][:5]:\n",
        "            model = meta.get('model', 'N/A')\n",
        "            trim = meta.get('trim', '')\n",
        "            mpg = meta.get('mpg_combined', 'N/A')\n",
        "            print(f\"     - {model} {trim}: {mpg} MPG\")\n",
        "except Exception as e:\n",
        "    print(f\"   Error: {e}\")\n",
        "\n",
        "# Test 3: Filter by model AND trim\n",
        "# Note: ChromaDB requires $and operator for multiple conditions\n",
        "print(\"\\n3. Filter: model='RAV4' AND trim='TRD Pro'\")\n",
        "results = vectorstore.similarity_search(\n",
        "    \"Toyota\", \n",
        "    k=5, \n",
        "    filter={\n",
        "        \"$and\": [\n",
        "            {\"model\": \"RAV4\"},\n",
        "            {\"trim\": \"TRD Pro\"}\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "if results:\n",
        "    print(f\"   Found {len(results)} matching chunk(s)\")\n",
        "    for doc in results:\n",
        "        print(f\"     Metadata: {doc.metadata}\")\n",
        "else:\n",
        "    print(\"   No matches found (check if this trim exists)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Metadata filtering validation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 14: Validation - Hybrid Query\n",
        "\n",
        "Test combined semantic search + metadata filtering, simulating Notebook1's planning queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "HYBRID QUERY VALIDATION\n",
            "============================================================\n",
            "\n",
            "Query: 'Which Toyota is comfortable for families under $30,000?'\n",
            "Approach: Semantic search + price filter (post-processing)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Semantic search returned 10 chunks\n",
            "\n",
            "âœ“ Found 4 affordable family-friendly options:\n",
            "------------------------------------------------------------\n",
            "\n",
            "  Corolla XSE\n",
            "    Price: $27,000\n",
            "    MPG: N/A\n",
            "    Seats: N/A\n",
            "\n",
            "  Camry Hybrid\n",
            "    Price: $29,000\n",
            "    MPG: N/A\n",
            "    Seats: N/A\n",
            "\n",
            "  Camry SE\n",
            "    Price: $28,000\n",
            "    MPG: N/A\n",
            "    Seats: N/A\n",
            "\n",
            "  Camry LE\n",
            "    Price: $26,000\n",
            "    MPG: N/A\n",
            "    Seats: N/A\n",
            "\n",
            "============================================================\n",
            "âœ… Hybrid query validation complete\n",
            "\n",
            "This demonstrates how Notebook1's planning queries will work!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"HYBRID QUERY VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simulate Notebook1 planning query\n",
        "print(\"\\nQuery: 'Which Toyota is comfortable for families under $30,000?'\")\n",
        "print(\"Approach: Semantic search + price filter (post-processing)\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Step 1: Semantic search\n",
        "results = vectorstore.similarity_search(\n",
        "    \"comfortable spacious family sedan reliable\",\n",
        "    k=10\n",
        ")\n",
        "\n",
        "print(f\"\\nSemantic search returned {len(results)} chunks\")\n",
        "\n",
        "# Step 2: Filter by price (post-processing for demo)\n",
        "affordable = []\n",
        "for doc in results:\n",
        "    price_str = doc.metadata.get('starting_price_mentions', '')\n",
        "    # Check for prices under $30,000\n",
        "    if any(p in price_str for p in ['26,000', '27,000', '28,000', '29,000']):\n",
        "        affordable.append(doc)\n",
        "\n",
        "print(f\"\\nâœ“ Found {len(affordable)} affordable family-friendly options:\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for doc in affordable[:5]:  # Show top 5\n",
        "    model = doc.metadata.get('model', 'N/A')\n",
        "    trim = doc.metadata.get('trim', '')\n",
        "    price = doc.metadata.get('starting_price_mentions', 'N/A')\n",
        "    mpg = doc.metadata.get('mpg_combined', 'N/A')\n",
        "    seats = doc.metadata.get('seats', 'N/A')\n",
        "    \n",
        "    print(f\"\\n  {model} {trim}\")\n",
        "    print(f\"    Price: {price}\")\n",
        "    print(f\"    MPG: {mpg}\")\n",
        "    print(f\"    Seats: {seats}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Hybrid query validation complete\")\n",
        "print(\"\\nThis demonstrates how Notebook1's planning queries will work!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 15: Ingestion Summary\n",
        "\n",
        "Final statistics and next steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INGESTION SUMMARY\n",
            "============================================================\n",
            "\n",
            "Total documents in collection: 35\n",
            "Sample chunk size: 3278 chars (~819 tokens)\n",
            "\n",
            "Metadata fields available:\n",
            "  - chunk_id\n",
            "  - drivetrain\n",
            "  - model\n",
            "  - mpg_city\n",
            "  - mpg_hwy\n",
            "  - source\n",
            "  - starting_price_mentions\n",
            "  - trim\n",
            "\n",
            "Processing summary:\n",
            "  - Introduction_to_Toyota_Car_Sales.pdf: 0 chunks\n",
            "  - Toyota_Camry_Specifications.pdf: 4 chunks\n",
            "  - Toyota_Corolla_Specifications.pdf: 5 chunks\n",
            "  - Toyota_Highlander_Specifications.pdf: 7 chunks\n",
            "  - Toyota_Prius_Specifications.pdf: 7 chunks\n",
            "  - Toyota_RAV4_Specifications.pdf: 7 chunks\n",
            "  - Toyota_Tacoma_Specifications.pdf: 3 chunks\n",
            "  - Toyota_bZ4X_Specifications.pdf: 2 chunks\n",
            "\n",
            "============================================================\n",
            "âœ… INGESTION COMPLETE AND VALIDATED!\n",
            "============================================================\n",
            "\n",
            "Next steps:\n",
            "  1. Run Notebook1_Prompt_to_Plan_Llama_ORDERED.ipynb\n",
            "  2. Test planning queries with semantic search + metadata filtering\n",
            "  3. Verify multi-query variants work with rich chunk content\n",
            "\n",
            "Chunk structure:\n",
            "  âœ… Full PDF text for semantic search\n",
            "  âœ… Structured specs for readability\n",
            "  âœ… KeySpec metadata for precise filtering\n",
            "  âœ… Supports ALL Notebook1 planning concepts\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"INGESTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Collection stats\n",
        "collection = vectorstore._collection\n",
        "total_docs = collection.count()\n",
        "print(f\"\\nTotal documents in collection: {total_docs}\")\n",
        "\n",
        "# Sample chunk size\n",
        "sample = vectorstore.get(limit=1)\n",
        "if sample['documents']:\n",
        "    sample_length = len(sample['documents'][0])\n",
        "    print(f\"Sample chunk size: {sample_length} chars (~{sample_length//4} tokens)\")\n",
        "\n",
        "# Metadata fields\n",
        "if sample['metadatas']:\n",
        "    print(f\"\\nMetadata fields available:\")\n",
        "    for key in sorted(sample['metadatas'][0].keys()):\n",
        "        print(f\"  - {key}\")\n",
        "\n",
        "# Processing summary\n",
        "print(f\"\\nProcessing summary:\")\n",
        "for pdf_info in ingestion_stats['pdfs_processed']:\n",
        "    print(f\"  - {pdf_info['filename']}: {pdf_info['chunks']} chunks\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… INGESTION COMPLETE AND VALIDATED!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Run Notebook1_Prompt_to_Plan_Llama_ORDERED.ipynb\")\n",
        "print(\"  2. Test planning queries with semantic search + metadata filtering\")\n",
        "print(\"  3. Verify multi-query variants work with rich chunk content\")\n",
        "print(\"\\nChunk structure:\")\n",
        "print(\"  âœ… Full PDF text for semantic search\")\n",
        "print(\"  âœ… Structured specs for readability\")\n",
        "print(\"  âœ… KeySpec metadata for precise filtering\")\n",
        "print(\"  âœ… Supports ALL Notebook1 planning concepts\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
