{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 Assignment: Build Your Toyota RAG System - SOLUTION\n",
        "\n",
        "**Instructor Reference Solution**\n",
        "\n",
        "This notebook contains complete implementations for all assignment tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pypdf\n",
        "import chromadb\n",
        "from langchain_google_vertexai import VertexAI\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(\"✓ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Load Toyota PDFs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pdf(pdf_path):\n",
        "    \"\"\"Load and extract text from a PDF file.\"\"\"\n",
        "    with open(pdf_path, 'rb') as f:\n",
        "        reader = pypdf.PdfReader(f)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Test\n",
        "data_dir = Path(\"../data/car-specs/toyota-specs\")\n",
        "test_pdf = data_dir / \"Toyota_Camry_Specifications.pdf\"\n",
        "text = load_pdf(test_pdf)\n",
        "\n",
        "assert len(text) > 2000, \"Should extract substantial text\"\n",
        "print(f\"✓ Loaded {len(text)} characters from {test_pdf.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Chunking Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_chunk(text, chunk_size=500, overlap=50):\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "        \n",
        "        start = end - overlap\n",
        "        \n",
        "        if start >= len(text) - overlap:\n",
        "            break\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Load all PDFs and chunk\n",
        "pdfs = sorted(data_dir.glob(\"*.pdf\"))\n",
        "all_chunks = []\n",
        "\n",
        "for pdf in pdfs:\n",
        "    text = load_pdf(pdf)\n",
        "    model = pdf.stem.replace(\"_\", \" \").replace(\" Specifications\", \"\")\n",
        "    chunks = simple_chunk(text, chunk_size=500, overlap=50)\n",
        "    \n",
        "    for i, chunk in enumerate(chunks):\n",
        "        all_chunks.append({\n",
        "            \"content\": chunk,\n",
        "            \"model\": model,\n",
        "            \"source\": pdf.name,\n",
        "            \"chunk_id\": f\"{pdf.name}_{i}\"\n",
        "        })\n",
        "\n",
        "print(f\"✓ Created {len(all_chunks)} chunks from {len(pdfs)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Store in ChromaDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = chromadb.Client()\n",
        "collection = client.create_collection(\n",
        "    name=\"toyota_specs_assignment\",\n",
        "    metadata={\"description\": \"Toyota specs - Assignment solution\"}\n",
        ")\n",
        "\n",
        "documents_list = [chunk[\"content\"] for chunk in all_chunks]\n",
        "metadatas = [{\"model\": chunk[\"model\"], \"source\": chunk[\"source\"]} for chunk in all_chunks]\n",
        "ids = [chunk[\"chunk_id\"] for chunk in all_chunks]\n",
        "\n",
        "collection.add(documents=documents_list, metadatas=metadatas, ids=ids)\n",
        "print(f\"✓ Stored {collection.count()} chunks in ChromaDB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Complete RAG Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = VertexAI(model_name=\"gemini-pro\", temperature=0)\n",
        "\n",
        "def ask_toyota_question(question, collection, llm):\n",
        "    \"\"\"Ask a question about Toyota using RAG.\"\"\"\n",
        "    results = collection.query(query_texts=[question], n_results=3)\n",
        "    context = \"\\\\n\\\\n\".join(results['documents'][0])\n",
        "    sources = results['metadatas'][0]\n",
        "    \n",
        "    prompt = f\"\"\"You are a helpful Toyota sales assistant. Answer based on the provided information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    answer = llm.invoke(prompt)\n",
        "    return answer, sources\n",
        "\n",
        "# Test\n",
        "answer, sources = ask_toyota_question(\"What's the Camry's horsepower?\", collection, llm)\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Sources: {[s['model'] for s in sources]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Reflection Questions\n",
        "\n",
        "### Question 1: Chunking Strategy\n",
        "\n",
        "**Answer:** The simple 500-character chunking is easy to implement and creates consistent chunk sizes, but it has significant drawbacks. It can split mid-sentence or mid-paragraph, breaking semantic coherence. It might separate related information like an engine specification from its description. This approach would fail when dealing with structured sections like tables or when important context spans across the arbitrary boundary.\n",
        "\n",
        "### Question 2: Retrieval Quality\n",
        "\n",
        "**Answer:** Specification queries (e.g., \"What's the horsepower?\") worked best because they target specific factual information that's likely contained in a single chunk. Feature queries also performed well when the feature description was self-contained. General queries struggled more because they require synthesizing information across multiple sections or documents, and simple retrieval may miss relevant context.\n",
        "\n",
        "### Question 3: Improvements\n",
        "\n",
        "**Answer:** The most impactful improvement would be section-based chunking that respects document structure. This would ensure that complete thoughts and related information stay together, improving both retrieval accuracy and answer quality. Additionally, adding metadata filtering (by model, section type) would allow more precise retrieval for specific queries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
