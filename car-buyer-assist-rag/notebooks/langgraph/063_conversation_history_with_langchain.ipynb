{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conversation History with LangChain\n",
        "\n",
        "Understand how LLMs handle (and lose) conversation context, and how to manage it manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. **Understand LLM statelessness** - LLMs have no memory; each call is independent\n",
        "2. **Use LangChain message types** - `HumanMessage`, `AIMessage`, and `SystemMessage`\n",
        "3. **Manage conversation history** - Maintain context across multiple turns using a message list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\"../../.env\")\n",
        "print(\"✅ Environment loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "print(\"✅ All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(\"✅ LLM initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LLMs Are Stateless\n",
        "\n",
        "Each LLM call is completely independent. The model has **no memory** of previous calls.\n",
        "\n",
        "Let's prove this with a simple example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call 1: Tell the LLM something\n",
        "response1 = llm.invoke([HumanMessage(content=\"My name is Ravi and I live in Hyderabad.\")])\n",
        "print(f\"Call 1 Response:\\n{response1.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call 2: Ask about what we just said\n",
        "response2 = llm.invoke([HumanMessage(content=\"What is my name and where do I live?\")])\n",
        "print(f\"Call 2 Response:\\n{response2.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The LLM **does not remember** Call 1 when processing Call 2. Each call starts fresh with no context.\n",
        "\n",
        "This is a fundamental property of LLMs — they are stateless functions that only see what you pass in the current request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. LangChain Message Types\n",
        "\n",
        "LangChain provides three core message types to structure conversations:\n",
        "\n",
        "| Message Type | Purpose | Example |\n",
        "|---|---|---|\n",
        "| `SystemMessage` | Sets the LLM's behavior/role | \"You are a helpful assistant\" |\n",
        "| `HumanMessage` | User input | \"What is the capital of France?\" |\n",
        "| `AIMessage` | LLM's response | \"The capital of France is Paris.\" |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create each message type\n",
        "system_msg = SystemMessage(content=\"You are a helpful travel assistant.\")\n",
        "human_msg = HumanMessage(content=\"What is the capital of France?\")\n",
        "ai_msg = AIMessage(content=\"The capital of France is Paris.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the message structure\n",
        "for msg in [system_msg, human_msg, ai_msg]:\n",
        "    print(f\"Type: {msg.type:10s} | Content: {msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Passing Conversation History Manually\n",
        "\n",
        "The solution to statelessness is simple: **pass the entire conversation history** with every call.\n",
        "\n",
        "The LLM accepts a list of messages, so we include all previous messages in each request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the full conversation and pass it\n",
        "messages = [\n",
        "    HumanMessage(content=\"My name is Ravi and I live in Hyderabad.\"),\n",
        "    AIMessage(content=\"Nice to meet you, Ravi! Hyderabad is a wonderful city.\"),\n",
        "    HumanMessage(content=\"What is my name and where do I live?\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(f\"Response:\\n{response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the LLM **remembers** because we included the full conversation in the request.\n",
        "\n",
        "The LLM didn't actually \"remember\" anything — it simply read all the messages we passed and responded based on the full context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Building a Multi-Turn Conversation\n",
        "\n",
        "Let's build a proper multi-turn conversation by accumulating messages in a Python list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with a system message\n",
        "conversation = [\n",
        "    SystemMessage(content=\"You are a Toyota car sales assistant. Keep responses brief and helpful.\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn 1\n",
        "conversation.append(HumanMessage(content=\"I'm looking for a family SUV. What do you recommend?\"))\n",
        "\n",
        "response = llm.invoke(conversation)\n",
        "conversation.append(response)\n",
        "\n",
        "print(f\"Turn 1 ({len(conversation)} messages):\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn 2 - Reference previous context\n",
        "conversation.append(HumanMessage(content=\"What's the price of the first one you mentioned?\"))\n",
        "\n",
        "response = llm.invoke(conversation)\n",
        "conversation.append(response)\n",
        "\n",
        "print(f\"Turn 2 ({len(conversation)} messages):\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn 3 - Use pronoun referencing\n",
        "conversation.append(HumanMessage(content=\"Does it come in a hybrid version?\"))\n",
        "\n",
        "response = llm.invoke(conversation)\n",
        "conversation.append(response)\n",
        "\n",
        "print(f\"Turn 3 ({len(conversation)} messages):\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The LLM correctly resolves:\n",
        "- **\"the first one\"** → refers to the SUV from Turn 1\n",
        "- **\"it\"** → refers to the car being discussed in Turn 2\n",
        "\n",
        "This works because we pass **all previous messages** on every call."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inspecting the Conversation State\n",
        "\n",
        "Let's look at what our conversation list contains after 3 turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Total messages in conversation: {len(conversation)}\")\n",
        "print()\n",
        "\n",
        "for i, msg in enumerate(conversation):\n",
        "    label = msg.type.upper()\n",
        "    preview = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n",
        "    print(f\"  [{i}] {label:8s} | {preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pattern:** Each turn adds 2 messages (1 Human + 1 AI), so the list grows as:\n",
        "\n",
        "```\n",
        "Start:  1 message  (SystemMessage)\n",
        "Turn 1: 3 messages (+1 Human, +1 AI)\n",
        "Turn 2: 5 messages (+1 Human, +1 AI)\n",
        "Turn 3: 7 messages (+1 Human, +1 AI)\n",
        "```\n",
        "\n",
        "This growing list **is** the conversation history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. The Challenge of Manual Management\n",
        "\n",
        "Managing conversation history manually works, but has drawbacks:\n",
        "\n",
        "1. **You must remember to append** every Human and AI message\n",
        "2. **The list grows indefinitely** — no built-in size management\n",
        "3. **Tool messages add complexity** — when tools are involved, you need to track `ToolMessage` responses too\n",
        "4. **Easy to make mistakes** — forgetting to append a message breaks the context\n",
        "\n",
        "In the next notebook, we'll see how **LangGraph's `MessagesState`** solves these problems by automatically managing message accumulation within a graph workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "### What You've Accomplished\n",
        "\n",
        "✅ **Understood LLM statelessness** — Each LLM call is independent with no built-in memory\n",
        "\n",
        "✅ **Used LangChain message types** — `HumanMessage`, `AIMessage`, and `SystemMessage` to structure conversations\n",
        "\n",
        "✅ **Managed conversation history** — Built multi-turn conversations by accumulating messages in a list\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "**LLMs don't remember — you pass the memory.** Every call requires the full conversation history to maintain context. This is the fundamental pattern that all conversation systems (including LangGraph) build upon.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Continue to **Notebook: Graph Construction** to learn how LangGraph's `MessagesState` automates this message management within a graph workflow."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cbag-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbformat_minor": 4,
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
