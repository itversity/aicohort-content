{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph Graph Construction: Nodes & Edges\n",
        "\n",
        "Build your first LangGraph workflow by connecting nodes and edges into a graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. **Understand `MessagesState`** \u2014 How LangGraph automates the conversation history you managed manually in the previous notebook\n",
        "2. **Define nodes** \u2014 Create the LLM node and tool node that perform the work\n",
        "3. **Connect edges** \u2014 Wire nodes together with edges and conditional edges to control the flow\n",
        "4. **Visualize the graph** \u2014 See the complete workflow as a diagram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\"../../.env\")\n",
        "print(\"\u2705 Environment loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from typing import Literal\n",
        "\n",
        "print(\"\u2705 All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mermaid helper for graph visualization\n",
        "def render_mermaid(diagram_code, width=400):\n",
        "    \"\"\"Render Mermaid diagrams using mermaid.ink service.\"\"\"\n",
        "    from IPython.display import Image, display\n",
        "    import base64\n",
        "    \n",
        "    graphbytes = diagram_code.encode('utf-8')\n",
        "    base64_bytes = base64.urlsafe_b64encode(graphbytes)\n",
        "    base64_string = base64_bytes.decode('ascii')\n",
        "    url = f'https://mermaid.ink/img/{base64_string}'\n",
        "    display(Image(url=url, width=width))\n",
        "\n",
        "print(\"\u2705 Visualization helper defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. From Manual History to MessagesState\n",
        "\n",
        "In the previous notebook, we managed conversation history manually using a Python list:\n",
        "\n",
        "```python\n",
        "conversation = []\n",
        "conversation.append(HumanMessage(content=\"...\"))\n",
        "response = llm.invoke(conversation)\n",
        "conversation.append(response)  # Must remember to do this!\n",
        "```\n",
        "\n",
        "LangGraph's `MessagesState` automates this. It is essentially a typed dictionary with a single key `\"messages\"` that **automatically accumulates** messages as they flow through the graph.\n",
        "\n",
        "```python\n",
        "# MessagesState is equivalent to:\n",
        "{\"messages\": [HumanMessage, AIMessage, ToolMessage, ...]}\n",
        "```\n",
        "\n",
        "When a node returns `{\"messages\": [new_message]}`, LangGraph **appends** it to the existing list rather than replacing it. This is the key difference \u2014 you no longer need to manage the list yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Tools\n",
        "\n",
        "We'll reuse the tools from our earlier notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def currency_converter(amount: float, from_currency: str, to_currency: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert currency from one type to another.\n",
        "    \n",
        "    Use this tool when users need to convert monetary amounts between\n",
        "    different currencies. Supports USD, EUR, GBP, INR, and JPY.\n",
        "    \"\"\"\n",
        "    exchange_rates = {\"USD\": 1.0, \"EUR\": 0.92, \"GBP\": 0.79, \"INR\": 83.12, \"JPY\": 149.50}\n",
        "    \n",
        "    from_currency = from_currency.upper()\n",
        "    to_currency = to_currency.upper()\n",
        "    \n",
        "    if from_currency not in exchange_rates:\n",
        "        return f\"Error: Unsupported currency {from_currency}\"\n",
        "    if to_currency not in exchange_rates:\n",
        "        return f\"Error: Unsupported currency {to_currency}\"\n",
        "    \n",
        "    amount_in_usd = amount / exchange_rates[from_currency]\n",
        "    converted_amount = amount_in_usd * exchange_rates[to_currency]\n",
        "    effective_rate = exchange_rates[to_currency] / exchange_rates[from_currency]\n",
        "    \n",
        "    return (\n",
        "        f\"Conversion Result:\\n\"\n",
        "        f\"  {amount:,.2f} {from_currency} = {converted_amount:,.2f} {to_currency}\\n\"\n",
        "        f\"  Exchange Rate: 1 {from_currency} = {effective_rate:.4f} {to_currency}\"\n",
        "    )\n",
        "\n",
        "@tool\n",
        "def emi_calculator(principal: float, annual_interest_rate: float, tenure_months: int, currency: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculate the EMI (Equated Monthly Installment) for a loan.\n",
        "    \n",
        "    Use this tool when users want to know their monthly loan payment,\n",
        "    total repayment amount, or total interest for a loan.\n",
        "    \"\"\"\n",
        "    if principal <= 0:\n",
        "        return \"Error: Principal must be greater than 0\"\n",
        "    if annual_interest_rate < 0:\n",
        "        return \"Error: Interest rate cannot be negative\"\n",
        "    if tenure_months <= 0:\n",
        "        return \"Error: Tenure must be greater than 0\"\n",
        "    \n",
        "    monthly_interest_rate = annual_interest_rate / 12 / 100\n",
        "    \n",
        "    if monthly_interest_rate == 0:\n",
        "        emi = principal / tenure_months\n",
        "        total_payment = principal\n",
        "        total_interest = 0\n",
        "    else:\n",
        "        emi = principal * monthly_interest_rate * \\\n",
        "              pow(1 + monthly_interest_rate, tenure_months) / \\\n",
        "              (pow(1 + monthly_interest_rate, tenure_months) - 1)\n",
        "        total_payment = emi * tenure_months\n",
        "        total_interest = total_payment - principal\n",
        "    \n",
        "    return (\n",
        "        f\"EMI Calculation Result:\\n\"\n",
        "        f\"  Loan Amount: {principal:,.2f} {currency}\\n\"\n",
        "        f\"  Interest Rate: {annual_interest_rate}% per annum\\n\"\n",
        "        f\"  Tenure: {tenure_months} months\\n\"\n",
        "        f\"  Monthly EMI: {emi:,.2f} {currency}\\n\"\n",
        "        f\"  Total Payment: {total_payment:,.2f} {currency}\\n\"\n",
        "        f\"  Total Interest: {total_interest:,.2f} {currency}\"\n",
        "    )\n",
        "\n",
        "print(\"\u2705 Tools defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize LLM with Tools\n",
        "\n",
        "`bind_tools()` tells the LLM about the available tools so it can decide when to call them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "tools = [currency_converter, emi_calculator]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"\u2705 LLM initialized with tools\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Nodes\n",
        "\n",
        "A **node** is a function that takes the current state and returns an update to it.\n",
        "\n",
        "Our graph needs two nodes:\n",
        "\n",
        "| Node | Purpose | Implementation |\n",
        "|---|---|---|\n",
        "| `llm` | Calls the LLM to generate a response or request a tool call | Custom function |\n",
        "| `tools` | Executes the tool that the LLM requested | LangGraph's built-in `ToolNode` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Node 1: LLM node\n",
        "def call_llm(state: MessagesState):\n",
        "    \"\"\"LLM node that invokes the LLM with the current messages.\"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "print(\"\u2705 LLM node defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the pattern:\n",
        "- **Input:** `state[\"messages\"]` \u2014 reads the full message history from the state\n",
        "- **Output:** `{\"messages\": [response]}` \u2014 returns the new message to be **appended** to the state\n",
        "\n",
        "This is exactly what we did manually in the previous notebook, but now LangGraph handles the appending."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Node 2: Tool node (built-in)\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "print(\"\u2705 Tool node defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`ToolNode` is a prebuilt LangGraph node that:\n",
        "1. Reads the last AIMessage from state\n",
        "2. Extracts the `tool_calls` from it\n",
        "3. Executes the requested tool(s)\n",
        "4. Returns the result as a `ToolMessage`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Define the Router\n",
        "\n",
        "The **router** is a function that decides which node to go to next. It looks at the LLM's response and checks:\n",
        "- If the LLM wants to call a tool \u2192 route to the `tools` node\n",
        "- If the LLM has a final answer \u2192 route to `END`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
        "    \"\"\"Router that decides next step based on whether tool_calls exist.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "print(\"\u2705 Router function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build the Graph\n",
        "\n",
        "Now we connect everything together. A LangGraph workflow has three building blocks:\n",
        "\n",
        "| Component | Method | Purpose |\n",
        "|---|---|---|\n",
        "| **State** | `StateGraph(MessagesState)` | Defines the data structure flowing through the graph |\n",
        "| **Nodes** | `add_node(name, function)` | Processing steps that read/update state |\n",
        "| **Edges** | `add_edge()` / `add_conditional_edges()` | Connections that control the flow |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Create the graph with state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "print(\"\u2705 Graph created with MessagesState\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Add nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "workflow.add_node(\"llm\", call_llm)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "print(\"\u2705 Nodes added: 'llm' and 'tools'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Add edges\n",
        "\n",
        "We need three connections:\n",
        "\n",
        "1. **START \u2192 llm** \u2014 Every query begins at the LLM node\n",
        "2. **llm \u2192 tools OR END** \u2014 Conditional: depends on whether the LLM wants to call a tool\n",
        "3. **tools \u2192 llm** \u2014 After a tool executes, go back to the LLM to process the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edge 1: Entry point\n",
        "workflow.add_edge(START, \"llm\")\n",
        "\n",
        "print(\"\u2705 Edge added: START \u2192 llm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edge 2: Conditional - LLM decides the next step\n",
        "workflow.add_conditional_edges(\n",
        "    \"llm\",\n",
        "    should_continue,\n",
        "    {\"tools\": \"tools\", END: END}\n",
        ")\n",
        "\n",
        "print(\"\u2705 Conditional edge added: llm \u2192 tools OR END\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edge 3: Loop back after tool execution\n",
        "workflow.add_edge(\"tools\", \"llm\")\n",
        "\n",
        "print(\"\u2705 Edge added: tools \u2192 llm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The complete flow looks like this:\n",
        "\n",
        "```\n",
        "START \u2192 llm \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 END\n",
        "              \u2502                              \u25b2\n",
        "              \u2502 (has tool_calls?)             \u2502 (no tool_calls)\n",
        "              \u25bc                              \u2502\n",
        "            tools \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 llm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Compile and Visualize\n",
        "\n",
        "Compiling converts the workflow definition into an executable application. It validates that all nodes are reachable and all edges are properly connected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "app = workflow.compile()\n",
        "\n",
        "print(\"\u2705 Graph compiled successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the graph\n",
        "mermaid_diagram = app.get_graph().draw_mermaid()\n",
        "render_mermaid(mermaid_diagram, width=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Quick Test\n",
        "\n",
        "Let's do a quick test to confirm the graph works. We'll examine the execution in detail in the next notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a tool query\n",
        "result = app.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"Convert 100 USD to EUR\")]\n",
        "})\n",
        "\n",
        "print(f\"Total messages generated: {len(result['messages'])}\")\n",
        "print(f\"\\nFinal Response:\\n{result['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a non-tool query (goes directly to END)\n",
        "result = app.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"Hello, how are you?\")]\n",
        "})\n",
        "\n",
        "print(f\"Total messages generated: {len(result['messages'])}\")\n",
        "print(f\"\\nFinal Response:\\n{result['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the difference:\n",
        "- **Tool query** (\"Convert 100 USD to EUR\") \u2192 Multiple messages (LLM \u2192 Tool \u2192 LLM)\n",
        "- **Non-tool query** (\"Hello, how are you?\") \u2192 Only 2 messages (LLM \u2192 END directly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "### What You've Accomplished\n",
        "\n",
        "\u2705 **Understood `MessagesState`** \u2014 LangGraph's automatic message accumulation replaces manual list management\n",
        "\n",
        "\u2705 **Defined nodes** \u2014 Created an LLM node (custom function) and a tool node (`ToolNode`)\n",
        "\n",
        "\u2705 **Connected edges** \u2014 Wired the graph with `add_edge`, `add_conditional_edges`, `START`, and `END`\n",
        "\n",
        "\u2705 **Visualized the graph** \u2014 Used Mermaid to render the workflow diagram\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**Nodes** are functions that process state. **Edges** control flow between nodes. **Conditional edges** enable branching logic based on the current state.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Continue to **Notebook 065c: Graph Compilation & Testing** to understand the complete message flow and test the graph with different scenarios."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cbag-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbformat_minor": 4,
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
