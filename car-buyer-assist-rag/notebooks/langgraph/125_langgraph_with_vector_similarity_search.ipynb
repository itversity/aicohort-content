{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph with Vector Similarity Search\n",
        "\n",
        "Building intelligent agents that combine LangGraph workflows with semantic search capabilities.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. **Integrate vector stores with LangGraph** - Connect ChromaDB vector store to LangGraph agents to enable semantic search capabilities\n",
        "2. **Create semantic search tools** - Build custom tools that perform vector similarity search and return relevant document chunks\n",
        "3. **Combine multiple tools** - Design agents that can intelligently use both semantic search tools (for information retrieval) and calculation tools (for processing)\n",
        "4. **Handle complex queries** - Process user requests that require both searching a knowledge base and performing calculations on the retrieved information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from typing import Literal, List\n",
        "\n",
        "load_dotenv(\"../../.env\")\n",
        "print(\"âœ… Environment loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(\"âœ… LLM initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Connect to Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChromaDB and Embeddings setup\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from google.genai import types\n",
        "\n",
        "# Configuration\n",
        "PERSIST_DIR = \"../../chroma_db\"\n",
        "COLLECTION_NAME = \"toyota_specs\"\n",
        "EMBED_MODEL_ID = \"gemini-embedding-001\"\n",
        "\n",
        "# Initialize embeddings (uses GOOGLE_API_KEY from environment)\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
        "    model=EMBED_MODEL_ID,\n",
        "    output_dimensionality=768\n",
        ")\n",
        "\n",
        "# Connect to vectorstore\n",
        "vectorstore = Chroma(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding_function=embeddings_model,\n",
        "    persist_directory=PERSIST_DIR\n",
        ")\n",
        "\n",
        "print(f\"âœ… Connected to vectorstore: {COLLECTION_NAME}\")\n",
        "print(f\"âœ… Using embedding model: {EMBED_MODEL_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Vector Similarity Search Helper\n",
        "\n",
        "Create a helper function to perform semantic search against the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector similarity search helper\n",
        "def vector_similarity_search(\n",
        "    query: str, \n",
        "    vectorstore, \n",
        "    k: int = 5\n",
        ") -> List[str]:\n",
        "    \"\"\"Perform vector similarity search.\"\"\"\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "    return [doc.page_content for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = vector_similarity_search(\n",
        "    \"What is the base price of the Toyota Camry?\", \n",
        "    vectorstore, \n",
        "    k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define LangGraph Tools\n",
        "\n",
        "Create tools that wrap our vector search and EMI calculator functionalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool 1: Vehicle Search\n",
        "@tool\n",
        "def search_vehicles(query: str, max_results: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Search Toyota vehicle database using semantic similarity.\n",
        "    \n",
        "    Use this tool when users need information about Toyota vehicles,\n",
        "    including specifications, pricing, fuel efficiency, or comparisons.\n",
        "    \n",
        "    Args:\n",
        "        query: Natural language search query about Toyota vehicles\n",
        "        max_results: Maximum number of results to return (default: 5)\n",
        "    \n",
        "    Returns:\n",
        "        Formatted string with vehicle information\n",
        "    \"\"\"\n",
        "    docs = vector_similarity_search(query, vectorstore, k=max_results)\n",
        "    \n",
        "    result = \"Vehicle Search Results:\\n\"\n",
        "    result += \"=\" * 60 + \"\\n\"\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        result += f\"\\nResult {i}:\\n{doc}\\n\"\n",
        "    result += \"=\" * 60\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"âœ… search_vehicles tool defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_vehicles.invoke({\n",
        "    \"query\": \"Which Toyota sedan is most fuel-efficient under $30,000?\",\n",
        "    \"max_results\": 3\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def emi_calculator(principal: float, annual_interest_rate: float, tenure_months: int, currency: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculate the EMI (Equated Monthly Installment) for a loan.\n",
        "    \n",
        "    Use this tool when users want to know their monthly loan payment,\n",
        "    total repayment amount, or total interest for a loan.\n",
        "    \n",
        "    Args:\n",
        "        principal: The loan amount\n",
        "        annual_interest_rate: Annual interest rate as percentage (e.g., 8.5)\n",
        "        tenure_months: Loan tenure in months\n",
        "        currency: Currency code (USD, EUR, GBP, INR, JPY)\n",
        "    \"\"\"\n",
        "    if principal <= 0 or annual_interest_rate < 0 or tenure_months <= 0:\n",
        "        return \"Error: Invalid input parameters\"\n",
        "    \n",
        "    monthly_interest_rate = annual_interest_rate / 12 / 100\n",
        "    \n",
        "    if monthly_interest_rate == 0:\n",
        "        emi = principal / tenure_months\n",
        "        total_payment = principal\n",
        "        total_interest = 0\n",
        "    else:\n",
        "        emi = principal * monthly_interest_rate * \\\n",
        "              pow(1 + monthly_interest_rate, tenure_months) / \\\n",
        "              (pow(1 + monthly_interest_rate, tenure_months) - 1)\n",
        "        total_payment = emi * tenure_months\n",
        "        total_interest = total_payment - principal\n",
        "    \n",
        "    return (\n",
        "        f\"EMI Calculation Result:\\n\"\n",
        "        f\"  Loan Amount: {principal:,.2f} {currency}\\n\"\n",
        "        f\"  Interest Rate: {annual_interest_rate}% per annum\\n\"\n",
        "        f\"  Tenure: {tenure_months} months\\n\"\n",
        "        f\"  Monthly EMI: {emi:,.2f} {currency}\\n\"\n",
        "        f\"  Total Payment: {total_payment:,.2f} {currency}\\n\"\n",
        "        f\"  Total Interest: {total_interest:,.2f} {currency}\"\n",
        "    )\n",
        "\n",
        "print(\"âœ… emi_calculator tool defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build LangGraph Workflow\n",
        "\n",
        "Construct the agent graph with LLM node, tool node, and routing logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build LangGraph workflow\n",
        "tools = [search_vehicles, emi_calculator]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def call_llm(state: MessagesState):\n",
        "    \"\"\"LLM node: Calls LLM with current messages.\"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
        "    \"\"\"Router: Check if agent wants to use tools.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# Build graph\n",
        "workflow = StateGraph(MessagesState)\n",
        "workflow.add_node(\"llm\", call_llm)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "workflow.add_edge(START, \"llm\")\n",
        "workflow.add_conditional_edges(\"llm\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "workflow.add_edge(\"tools\", \"llm\")\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"âœ… Graph compiled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test the Agent\n",
        "\n",
        "Execute various queries to test the agent's capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 1: Simple Vehicle Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Simple vehicle search\n",
        "state = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"Which Toyota sedan is most fuel-efficient under $30,000?\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Query: Which Toyota sedan is most fuel-efficient under $30,000?\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "result = app.invoke(state)\n",
        "\n",
        "print(f\"\\nTotal messages: {len(result['messages'])}\")\n",
        "print(\"\\nFinal Response:\")\n",
        "print(\"=\" * 70)\n",
        "print(result['messages'][-1].content)\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Combined Search and Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Comparison query\n",
        "state2 = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"\"\"\n",
        "                     What is the base price of the Toyota Camry \n",
        "                     and what is the EMI for $30,000 loan at 8.5% per annum for 36 months?\n",
        "                    \"\"\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"\"\"What is the base price of the Toyota Camry \n",
        "         and what is the EMI for $30,000 loan at 8.5% per annum for 36 months?\n",
        "    \"\"\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "result2 = app.invoke(state2)\n",
        "\n",
        "print(f\"\\nTotal messages: {len(result2['messages'])}\")\n",
        "print(\"\\nFinal Response:\")\n",
        "print(\"=\" * 70)\n",
        "print(result2['messages'][-1].content)\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result2[\"messages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result2[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Streaming Execution\n",
        "\n",
        "Observe the agent's decision-making process in real-time through streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming execution\n",
        "state_stream = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"Show me affordable SUVs with good fuel economy\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Streaming Execution\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Query: Show me affordable SUVs with good fuel economy\\n\")\n",
        "\n",
        "step_count = 0\n",
        "\n",
        "for event in app.stream(state_stream):\n",
        "    for node_name, data in event.items():\n",
        "        step_count += 1\n",
        "        print(f\"\\n[Step {step_count}] Node: '{node_name}'\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        if \"messages\" in data:\n",
        "            for msg in data[\"messages\"]:\n",
        "                if isinstance(msg, AIMessage):\n",
        "                    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "                        print(f\"  ðŸ” Calling {msg.tool_calls[0]['name']}\")\n",
        "                        print(f\"     Query: {msg.tool_calls[0]['args'].get('query')}\")\n",
        "                    else:\n",
        "                        print(f\"  ðŸ’¬ Final response generated\")\n",
        "                        \n",
        "                elif isinstance(msg, ToolMessage):\n",
        "                    print(f\"  âœ… Tool executed\")\n",
        "                    first_line = msg.content.split('\\n')[0]\n",
        "                    print(f\"     Result: {first_line}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Total steps: {step_count}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "✅ **Vector store integration** - Connected ChromaDB to LangGraph agents, enabling semantic search over Toyota vehicle specifications\n",
        "\n",
        "✅ **Semantic search tools** - Created a `search_vehicles` tool that performs vector similarity search and returns relevant document chunks based on natural language queries\n",
        "\n",
        "✅ **Multi-tool agents** - Built agents that intelligently combine semantic search (for retrieving vehicle information) with calculation tools (for EMI computation)\n",
        "\n",
        "✅ **Complex query handling** - Processed queries requiring both information retrieval from the knowledge base and mathematical calculations, demonstrating the power of combining RAG with agentic workflows\n",
        "\n",
        "### Key Patterns Demonstrated\n",
        "\n",
        "- **Parallel execution** - Agent can call the search tool to gather information while simultaneously being ready to perform calculations\n",
        "- **Sequential execution** - Agent first searches for vehicle price, then uses that information to calculate EMI\n",
        "- **Conversational context** - Each interaction maintains context through the MessagesState, enabling follow-up questions\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Now that you understand vector similarity search with LangGraph, you're ready to explore more **advanced patterns in detail**:\n",
        "\n",
        "- **Sequential execution patterns** - Deep dive into multi-step dependent workflows where each tool's output feeds into the next\n",
        "- **Conversational context management** - Learn how to maintain and leverage conversation history across multiple user turns for more natural interactions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
