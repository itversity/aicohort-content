{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toyota Specifications: PDF Ingestion with Text Splitting\n",
        "\n",
        "Extract and embed Toyota PDFs into ChromaDB with proper chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "load_dotenv(\"../../.env\")\n",
        "print(\"✅ Environment loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_DIR = \"../../data/toyota-specs\"\n",
        "PERSIST_DIR = \"../../chroma_db\"\n",
        "COLLECTION_NAME = \"toyota_specs\"\n",
        "\n",
        "print(f\"DATA_DIR: {DATA_DIR}\")\n",
        "print(f\"PERSIST_DIR: {PERSIST_DIR}\")\n",
        "print(f\"COLLECTION: {COLLECTION_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load PDFs\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_files = sorted(Path(DATA_DIR).glob(\"*.pdf\"))\n",
        "print(f\"Found {len(pdf_files)} PDFs:\")\n",
        "\n",
        "documents = []\n",
        "for pdf_path in pdf_files:\n",
        "    print(f\"  Loading {pdf_path.name}...\")\n",
        "    loader = PyPDFLoader(str(pdf_path))\n",
        "    docs = loader.load()\n",
        "    \n",
        "    # Add source metadata\n",
        "    for doc in docs:\n",
        "        doc.metadata[\"source\"] = pdf_path.name\n",
        "    \n",
        "    documents.extend(docs)\n",
        "\n",
        "print(f\"\\n✅ Loaded {len(documents)} pages from {len(pdf_files)} PDFs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into chunks\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"✅ Split into {len(chunks)} chunks\")\n",
        "print(f\"   Chunk size: 1000 characters\")\n",
        "print(f\"   Chunk overlap: 200 characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Embeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"gemini-embedding-001\",\n",
        "    output_dimensionality=768\n",
        ")\n",
        "print(\"✅ Using: gemini-embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete existing collection\n",
        "try:\n",
        "    vectorstore = Chroma(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding_function=embeddings,\n",
        "        persist_directory=PERSIST_DIR\n",
        "    )\n",
        "    vectorstore.delete_collection()\n",
        "    print(\"✅ Deleted existing collection\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error deleting collection: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ChromaDB Collection\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    persist_directory=PERSIST_DIR\n",
        ")\n",
        "\n",
        "print(f\"✅ Created collection: {COLLECTION_NAME}\")\n",
        "print(f\"   Total chunks: {len(chunks)}\")\n",
        "print(f\"   Persist directory: {PERSIST_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify collection\n",
        "count = vectorstore._collection.count()\n",
        "print(f\"Collection count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Search\n",
        "query = \"fuel efficient sedan\"\n",
        "results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "print(f\"Test query: '{query}'\\n\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"[{i}] {doc.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"    {doc.page_content[:150]}...\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cbag-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
