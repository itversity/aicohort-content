{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sequential Tool Execution with Vector Search\n",
        "\n",
        "Understanding how LangGraph executes dependent tools where search results inform subsequent calculations.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. **Understand sequential dependencies** - Recognize when tasks have dependencies where one tool's output becomes another tool's input\n",
        "2. **Identify sequential patterns** - Recognize multiple separate AIMessages with tool_calls indicating sequential loops through the graph\n",
        "3. **Trace data flow** - Verify how the LLM extracts values from vector search results and uses them as parameters in calculation tools\n",
        "4. **Apply to RAG workflows** - Combine semantic search with computational tools to create practical applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from typing import Literal, List\n",
        "\n",
        "load_dotenv(\"../../.env\")\n",
        "print(\"\u2705 Environment loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(\"\u2705 LLM initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Connect to Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChromaDB Configuration\n",
        "PERSIST_DIR = \"../../chroma_db\"\n",
        "COLLECTION_NAME = \"toyota_specs\"\n",
        "EMBED_MODEL_ID = \"gemini-embedding-001\"\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
        "    model=EMBED_MODEL_ID,\n",
        "    output_dimensionality=768\n",
        ")\n",
        "\n",
        "# Connect to vectorstore\n",
        "vectorstore = Chroma(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding_function=embeddings_model,\n",
        "    persist_directory=PERSIST_DIR\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Connected to vectorstore: {COLLECTION_NAME}\")\n",
        "print(f\"   Total documents: {vectorstore._collection.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector similarity search helper\n",
        "def vector_similarity_search(\n",
        "    query: str, \n",
        "    vectorstore, \n",
        "    k: int = 5\n",
        ") -> List[str]:\n",
        "    \"\"\"Perform vector similarity search.\"\"\"\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "    return [doc.page_content for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool 1: Vehicle Search\n",
        "@tool\n",
        "def search_vehicles(query: str, max_results: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Search Toyota vehicle database using semantic similarity.\n",
        "    \n",
        "    Use this tool when users need information about Toyota vehicles,\n",
        "    including specifications, pricing, fuel efficiency, or comparisons.\n",
        "    \n",
        "    Args:\n",
        "        query: Natural language search query about Toyota vehicles\n",
        "        max_results: Maximum number of results to return (default: 5)\n",
        "    \n",
        "    Returns:\n",
        "        Formatted string with vehicle information\n",
        "    \"\"\"\n",
        "    docs = vector_similarity_search(query, vectorstore, k=max_results)\n",
        "    \n",
        "    result = \"Vehicle Search Results:\\n\"\n",
        "    result += \"=\" * 60 + \"\\n\"\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        result += f\"\\nResult {i}:\\n{doc}\\n\"\n",
        "    result += \"=\" * 60\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"\u2705 search_vehicles tool defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool 2: EMI Calculator\n",
        "@tool\n",
        "def emi_calculator(principal: float, annual_interest_rate: float, tenure_months: int, currency: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculate the EMI (Equated Monthly Installment) for a loan.\n",
        "    \n",
        "    Use this tool when users want to know their monthly loan payment,\n",
        "    total repayment amount, or total interest for a loan.\n",
        "    \n",
        "    Args:\n",
        "        principal: The loan amount\n",
        "        annual_interest_rate: Annual interest rate as percentage (e.g., 8.5)\n",
        "        tenure_months: Loan tenure in months\n",
        "        currency: Currency code (USD, EUR, GBP, INR, JPY)\n",
        "    \"\"\"\n",
        "    if principal <= 0 or annual_interest_rate < 0 or tenure_months <= 0:\n",
        "        return \"Error: Invalid input parameters\"\n",
        "    \n",
        "    monthly_interest_rate = annual_interest_rate / 12 / 100\n",
        "    \n",
        "    if monthly_interest_rate == 0:\n",
        "        emi = principal / tenure_months\n",
        "        total_payment = principal\n",
        "        total_interest = 0\n",
        "    else:\n",
        "        emi = principal * monthly_interest_rate * \\\n",
        "              pow(1 + monthly_interest_rate, tenure_months) / \\\n",
        "              (pow(1 + monthly_interest_rate, tenure_months) - 1)\n",
        "        total_payment = emi * tenure_months\n",
        "        total_interest = total_payment - principal\n",
        "    \n",
        "    return (\n",
        "        f\"EMI Calculation Result:\\n\"\n",
        "        f\"  Loan Amount: {principal:,.2f} {currency}\\n\"\n",
        "        f\"  Interest Rate: {annual_interest_rate}% per annum\\n\"\n",
        "        f\"  Tenure: {tenure_months} months\\n\"\n",
        "        f\"  Monthly EMI: {emi:,.2f} {currency}\\n\"\n",
        "        f\"  Total Payment: {total_payment:,.2f} {currency}\\n\"\n",
        "        f\"  Total Interest: {total_interest:,.2f} {currency}\"\n",
        "    )\n",
        "\n",
        "print(\"\u2705 emi_calculator tool defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build LangGraph Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM with tools\n",
        "tools = [search_vehicles, emi_calculator]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def call_llm(state: MessagesState):\n",
        "    \"\"\"LLM node: Calls LLM with current messages.\"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
        "    \"\"\"Router: Check if agent wants to use tools.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# Build graph\n",
        "workflow = StateGraph(MessagesState)\n",
        "workflow.add_node(\"llm\", call_llm)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "workflow.add_edge(START, \"llm\")\n",
        "workflow.add_conditional_edges(\"llm\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "workflow.add_edge(\"tools\", \"llm\")\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"\u2705 Graph compiled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Understanding Sequential Execution\n",
        "\n",
        "### What Makes Execution Sequential?\n",
        "\n",
        "Sequential execution occurs when tasks have **dependencies** - one task's output is needed as input for the next task.\n",
        "\n",
        "**Key Indicators in Queries:**\n",
        "- **\"then\"** - Explicitly signals sequential order (\"find price, then calculate EMI\")\n",
        "- **\"that amount/price\"** - References result from previous task\n",
        "- **\"for it/that\"** - Pronoun referring to previous result\n",
        "\n",
        "**Our Example Query:**\n",
        "\n",
        "*\"Find the base price of the Toyota Camry, **then** calculate the EMI for **that amount** in USD at 7% interest for 60 months\"*\n",
        "\n",
        "This query has a clear dependency:\n",
        "1. First: Search for Camry price\n",
        "2. Then: Use that price to calculate EMI\n",
        "\n",
        "The LLM cannot calculate EMI until it knows the price from the search!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Execute Sequential Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sequential test: Dependent tasks\n",
        "sequential_state = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"Find the base price of the Toyota Camry, then calculate the EMI for that amount in USD at 7% interest for 60 months\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"SEQUENTIAL EXECUTION TEST\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Query: Find Camry base price, THEN calculate its EMI\")\n",
        "print(\"\\nExpected Pattern: MULTIPLE SEPARATE AIMessages with tool_calls\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute sequential test\n",
        "sequential_result = app.invoke(sequential_state)\n",
        "print(f\"\\nTotal messages: {len(sequential_result['messages'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Verify Sequential Pattern\n",
        "\n",
        "In sequential execution, we expect:\n",
        "- **Multiple AIMessages** with tool_calls (not just one)\n",
        "- Each AIMessage represents a separate loop through the graph\n",
        "- Pattern: AIMessage\u2081 \u2192 ToolMessage\u2081 \u2192 AIMessage\u2082 \u2192 ToolMessage\u2082 \u2192 AIMessage\u2083 (final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify sequential execution\n",
        "tool_call_messages = [\n",
        "    msg for msg in sequential_result['messages']\n",
        "    if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls\n",
        "]\n",
        "\n",
        "print(\"SEQUENTIAL EXECUTION VERIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"AIMessages with tool_calls: {len(tool_call_messages)}\")\n",
        "\n",
        "if len(tool_call_messages) > 1:\n",
        "    print(\"\\n\ud83d\udd04 CONFIRMED: Sequential execution detected!\")\n",
        "    print(f\"   {len(tool_call_messages)} SEPARATE tool call requests\\n\")\n",
        "    \n",
        "    for i, msg in enumerate(tool_call_messages, 1):\n",
        "        print(f\"  Loop {i}: {msg.tool_calls[0]['name']}\")\n",
        "        print(f\"    Args: {msg.tool_calls[0]['args']}\\n\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f Single tool call message (might be parallel)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Trace Data Flow\n",
        "\n",
        "The key to sequential execution is how data flows between tools:\n",
        "1. **Search executes** \u2192 Returns vehicle pricing information\n",
        "2. **LLM extracts** \u2192 Parses \"$26,000\" from the search result\n",
        "3. **EMI called** \u2192 Uses extracted price as principal parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data flow\n",
        "print(\"DATA FLOW VERIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if len(tool_call_messages) >= 2:\n",
        "    # Get the vehicle search result\n",
        "    search_result = sequential_result['messages'][2]  # First ToolMessage\n",
        "    print(\"STEP 1 - Vehicle Search Result:\")\n",
        "    print(f\"  {search_result.content[:200]}...\\n\")\n",
        "    \n",
        "    # Get the EMI tool call\n",
        "    emi_call = sequential_result['messages'][3]  # Second AIMessage\n",
        "    print(\"STEP 2 - EMI Calculator Called With:\")\n",
        "    for key, value in emi_call.tool_calls[0]['args'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"\u2705 LLM extracted price from search and used it for EMI calculation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Examine Message Sequence\n",
        "\n",
        "Let's examine each message in the sequence to understand the flow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all messages\n",
        "sequential_result['messages']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Message 0: User query\n",
        "dict(sequential_result['messages'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Message 1: First AIMessage - calls search_vehicles\n",
        "dict(sequential_result['messages'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Message 2: First ToolMessage - search result\n",
        "dict(sequential_result['messages'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Message 3: Second AIMessage - calls emi_calculator with extracted price\n",
        "dict(sequential_result['messages'][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Message 4: Second ToolMessage - EMI result\n",
        "dict(sequential_result['messages'][4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Message 5: Third AIMessage - final response synthesizing both results\n",
        "dict(sequential_result['messages'][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Final Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show final response\n",
        "print(\"FINAL RESPONSE:\")\n",
        "print(\"=\" * 80)\n",
        "print(sequential_result['messages'][-1].content)\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Streaming Execution\n",
        "\n",
        "Watch the sequential loops happen in real-time through streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming view\n",
        "state_stream = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"Find the base price of the Toyota Camry, then calculate the EMI for that amount in USD at 7% interest for 60 months\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"STREAMING EXECUTION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Watch the MULTIPLE loops in sequential execution...\\n\")\n",
        "\n",
        "step_count = 0\n",
        "loop_count = 0\n",
        "\n",
        "for event in app.stream(state_stream):\n",
        "    for node_name, data in event.items():\n",
        "        step_count += 1\n",
        "        print(f\"\\n[Step {step_count}] Node: '{node_name}'\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        if \"messages\" in data:\n",
        "            for msg in data[\"messages\"]:\n",
        "                if isinstance(msg, AIMessage):\n",
        "                    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "                        loop_count += 1\n",
        "                        print(f\"  \ud83d\udd04 LOOP {loop_count}: Calling {msg.tool_calls[0]['name']}\")\n",
        "                        print(f\"     Args: {msg.tool_calls[0]['args']}\")\n",
        "                    else:\n",
        "                        print(f\"  \ud83d\udcac Final response generated\")\n",
        "                        \n",
        "                elif isinstance(msg, ToolMessage):\n",
        "                    print(f\"  \u2705 Tool executed\")\n",
        "                    first_line = msg.content.split('\\n')[0]\n",
        "                    print(f\"     Result: {first_line}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Total steps: {step_count}\")\n",
        "print(f\"Total loops (agent \u2192 tools): {loop_count}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Additional Example: Another Sequential Query\n",
        "\n",
        "Test another sequential pattern to reinforce understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Another sequential example\n",
        "state2 = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"What's the starting price for the RAV4 Hybrid? Then calculate monthly payments for that price at 5.9% over 72 months.\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"SECOND SEQUENTIAL TEST\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Query: RAV4 Hybrid price \u2192 EMI calculation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "result2 = app.invoke(state2)\n",
        "\n",
        "print(f\"\\nTotal messages: {len(result2['messages'])}\")\n",
        "print(\"\\nFinal Response:\")\n",
        "print(\"-\" * 80)\n",
        "print(result2['messages'][-1].content)\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "\u2705 **Sequential execution dependencies** - When tasks have dependencies (\"then\", \"that amount\"), the LLM executes tools in separate loops, using results from one tool to inform parameters for the next\n",
        "\n",
        "\u2705 **Sequential pattern recognition** - Multiple separate AIMessages with tool_calls (e.g., AIMessage\u2081 \u2192 ToolMessage\u2081 \u2192 AIMessage\u2082 \u2192 ToolMessage\u2082 \u2192 AIMessage\u2083) indicating 2+ loops through the graph, resulting in 6+ total messages\n",
        "\n",
        "\u2705 **Data flow tracing** - The LLM extracts specific values from ToolMessage content (e.g., \"$26,000\" from search results) and uses them as parameters in subsequent tool calls (e.g., `principal=26000` for EMI calculation)\n",
        "\n",
        "\u2705 **RAG + Computation pattern** - Combined vector similarity search (information retrieval) with financial calculations (processing) to create a practical car-buying assistant that handles dependent tasks intelligently\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "**Message Count**: Sequential execution typically produces 6+ messages (vs 5 for parallel) due to multiple loops:\n",
        "- HumanMessage (query)\n",
        "- AIMessage\u2081 (call tool 1)\n",
        "- ToolMessage\u2081 (tool 1 result)\n",
        "- AIMessage\u2082 (call tool 2 with extracted data)\n",
        "- ToolMessage\u2082 (tool 2 result)\n",
        "- AIMessage\u2083 (final synthesis)\n",
        "\n",
        "**LLM Intelligence**: The LLM demonstrates semantic understanding by:\n",
        "- Recognizing dependencies in natural language (\"then\", \"that amount\")\n",
        "- Parsing and extracting relevant values from unstructured text\n",
        "- Mapping extracted values to correct tool parameters\n",
        "\n",
        "**Real-World Applications**:\n",
        "- E-commerce: Search for product \u2192 Calculate shipping cost\n",
        "- Real estate: Find property \u2192 Calculate mortgage\n",
        "- Travel: Search flights \u2192 Estimate total trip cost\n",
        "- Healthcare: Look up medication \u2192 Calculate dosage\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Now that you understand sequential execution, you're ready to explore **conversational context management** where the agent maintains state across multiple user turns for natural multi-turn interactions."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cbag-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}