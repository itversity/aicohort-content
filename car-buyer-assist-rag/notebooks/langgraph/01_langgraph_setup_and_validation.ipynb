{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph Tutorial: Setup & Validation\n",
        "\n",
        "## Objective\n",
        "Set up your development environment for LangGraph and validate connectivity to the LLM provider.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Required dependencies for LangGraph development\n",
        "2. Environment variable configuration\n",
        "3. LLM initialization with Google's Gemini\n",
        "4. Connectivity testing and validation\n",
        "5. Optional: LangSmith observability setup\n",
        "\n",
        "## Prerequisites\n",
        "- Python 3.10+ installed\n",
        "- A Google AI Studio API key (free tier available)\n",
        "- Basic familiarity with virtual environments\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Installation\n",
        "\n",
        "### Required Packages\n",
        "\n",
        "Run this command in your terminal to install all dependencies:\n",
        "\n",
        "```bash\n",
        "pip install langchain langchain-google-genai langchain-core langgraph python-dotenv\n",
        "```\n",
        "\n",
        "### Reference Point: Package Purposes\n",
        "\n",
        "| Package | Purpose |\n",
        "|---------|--------|\n",
        "| `langchain` | Core framework for building LLM applications |\n",
        "| `langchain-google-genai` | Google Gemini integration |\n",
        "| `langchain-core` | Core abstractions (tools, messages, etc.) |\n",
        "| `langgraph` | Graph-based agent workflows |\n",
        "| `python-dotenv` | Load environment variables from `.env` files |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Environment Configuration\n",
        "\n",
        "### Create a `.env` File\n",
        "\n",
        "Create a `.env` file in your project root with the following content:\n",
        "\n",
        "```env\n",
        "# ===========================================\n",
        "# REQUIRED: Google AI Configuration\n",
        "# ===========================================\n",
        "# Get your API key from: https://aistudio.google.com/apikey\n",
        "GOOGLE_API_KEY=your-google-api-key-here\n",
        "\n",
        "# ===========================================\n",
        "# OPTIONAL: LangSmith Observability\n",
        "# ===========================================\n",
        "# Get your API key from: https://smith.langchain.com\n",
        "LANGSMITH_API_KEY=your-langsmith-key\n",
        "LANGSMITH_PROJECT=langgraph-tutorial\n",
        "LANGCHAIN_TRACING_V2=true\n",
        "```\n",
        "\n",
        "### Reference Point: Getting Your API Key\n",
        "\n",
        "1. Go to [Google AI Studio](https://aistudio.google.com/apikey)\n",
        "2. Sign in with your Google account\n",
        "3. Click \"Create API Key\"\n",
        "4. Copy the key and paste it in your `.env` file\n",
        "\n",
        "> **Security Note:** Never commit your `.env` file to version control. Add `.env` to your `.gitignore` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Import Dependencies\n",
        "\n",
        "Let's import all the packages we'll need for this tutorial series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# LLM Provider\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Environment management\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"‚úÖ All imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference Point: Import Categories\n",
        "\n",
        "As we progress through the tutorial, we'll add more imports:\n",
        "\n",
        "| Category | Imports | Used In |\n",
        "|----------|---------|--------|\n",
        "| **LLM** | `ChatGoogleGenerativeAI` | This notebook |\n",
        "| **Tools** | `tool` from `langchain_core.tools` | Notebook 02, 03, 04 |\n",
        "| **Graph** | `StateGraph`, `MessagesState` from `langgraph` | Future notebooks |\n",
        "| **Messages** | `HumanMessage`, `AIMessage` from `langchain_core` | Future notebooks |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Load Environment Variables\n",
        "\n",
        "Load the variables from your `.env` file into the runtime environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Environment file loaded\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables from .env file\n",
        "# Adjust the path based on your .env file location\n",
        "load_dotenv(\"../../.env\")  # Change this path if needed\n",
        "\n",
        "print(\"‚úÖ Environment file loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment Variables Status:\n",
            "==================================================\n",
            "\n",
            "Required:\n",
            "   ‚úÖ GOOGLE_API_KEY: AIza...UgrE\n",
            "\n",
            "Optional (LangSmith):\n",
            "   ‚úÖ LANGSMITH_API_KEY: lsv2...dee0\n",
            "   ‚úÖ LANGSMITH_PROJECT: car-...-rag\n",
            "   ‚ö†Ô∏è  LANGCHAIN_TRACING_V2: Disabled\n",
            "\n",
            "==================================================\n",
            "‚úÖ Required configuration complete!\n"
          ]
        }
      ],
      "source": [
        "# Validate required environment variables\n",
        "def check_env_variable(var_name, required=True):\n",
        "    \"\"\"Check if an environment variable is set.\"\"\"\n",
        "    value = os.getenv(var_name)\n",
        "    if value:\n",
        "        # Mask the value for security (show first 4 and last 4 chars)\n",
        "        if len(value) > 12:\n",
        "            masked = f\"{value[:4]}...{value[-4:]}\"\n",
        "        else:\n",
        "            masked = \"****\"\n",
        "        print(f\"   ‚úÖ {var_name}: {masked}\")\n",
        "        return True\n",
        "    else:\n",
        "        if required:\n",
        "            print(f\"   ‚ùå {var_name}: NOT SET (Required!)\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  {var_name}: Not set (Optional)\")\n",
        "        return False\n",
        "\n",
        "print(\"Environment Variables Status:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check required variables\n",
        "print(\"\\nRequired:\")\n",
        "api_key_ok = check_env_variable(\"GOOGLE_API_KEY\", required=True)\n",
        "\n",
        "# Check optional variables\n",
        "print(\"\\nOptional (LangSmith):\")\n",
        "check_env_variable(\"LANGSMITH_API_KEY\", required=False)\n",
        "check_env_variable(\"LANGSMITH_PROJECT\", required=False)\n",
        "\n",
        "tracing = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "print(f\"   {'‚úÖ' if tracing else '‚ö†Ô∏è '} LANGCHAIN_TRACING_V2: {'Enabled' if tracing else 'Disabled'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "if api_key_ok:\n",
        "    print(\"‚úÖ Required configuration complete!\")\n",
        "else:\n",
        "    print(\"‚ùå Missing required configuration. Please check your .env file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference Point: Troubleshooting Environment Issues\n",
        "\n",
        "| Problem | Solution |\n",
        "|---------|----------|\n",
        "| `GOOGLE_API_KEY: NOT SET` | Check `.env` file path in `load_dotenv()` call |\n",
        "| File not found | Verify `.env` file exists and path is correct |\n",
        "| Key shows but LLM fails | Verify API key is valid at [AI Studio](https://aistudio.google.com) |\n",
        "| Import errors | Run `pip install` command from Section 1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: Initialize the LLM\n",
        "\n",
        "Now let's create our LLM instance using Google's Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM initialized successfully\n",
            "   Model: gemini-2.0-flash\n",
            "   Temperature: 0.3\n",
            "   Max Tokens: 1024\n"
          ]
        }
      ],
      "source": [
        "# Create the LLM instance\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LLM initialized successfully\")\n",
        "print(f\"   Model: gemini-2.0-flash\")\n",
        "print(f\"   Temperature: 0.3\")\n",
        "print(f\"   Max Tokens: 1024\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference Point: LLM Configuration Options\n",
        "\n",
        "| Parameter | Value | Purpose |\n",
        "|-----------|-------|--------|\n",
        "| `model` | `gemini-2.0-flash` | Fast, capable model suitable for tool use |\n",
        "| `temperature` | `0.3` | Lower = more deterministic responses (good for tools) |\n",
        "| `max_tokens` | `1024` | Maximum response length |\n",
        "\n",
        "### Available Gemini Models\n",
        "\n",
        "| Model | Best For |\n",
        "|-------|----------|\n",
        "| `gemini-2.0-flash` | Fast responses, good for most tasks |\n",
        "| `gemini-2.5-pro` | Complex reasoning, longer context |\n",
        "| `gemini-2.5-flash` | Balance of speed and capability |\n",
        "\n",
        "> **Tip:** Start with `gemini-2.0-flash` for development (faster, cheaper), then upgrade to `gemini-2.5-pro` for production if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Test LLM Connectivity\n",
        "\n",
        "Let's verify the LLM is working correctly by sending a simple test message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "LLM Connectivity Test\n",
            "==================================================\n",
            "\n",
            "üß™ Test Prompt: 'Say Hello from LangGraph!'\n",
            "\n",
            "ü§ñ Response: Hello from LangGraph!\n",
            "\n",
            "==================================================\n",
            "‚úÖ LLM connectivity verified!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"LLM Connectivity Test\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    # Send a simple test message\n",
        "    test_response = llm.invoke(\"Say 'Hello from LangGraph!' if you can read this.\")\n",
        "    \n",
        "    print(f\"\\nüß™ Test Prompt: 'Say Hello from LangGraph!'\")\n",
        "    print(f\"\\nü§ñ Response: {test_response.content}\")\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"‚úÖ LLM connectivity verified!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {type(e).__name__}\")\n",
        "    print(f\"   Message: {e}\")\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"‚ùå LLM connectivity failed. See troubleshooting below.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference Point: Common Connectivity Errors\n",
        "\n",
        "| Error | Cause | Solution |\n",
        "|-------|-------|----------|\n",
        "| `AuthenticationError` | Invalid API key | Regenerate key at AI Studio |\n",
        "| `RateLimitError` | Too many requests | Wait and retry, or upgrade plan |\n",
        "| `ConnectionError` | Network issues | Check internet connection |\n",
        "| `InvalidRequestError` | Malformed request | Check model name spelling |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7: Understanding the Response Object\n",
        "\n",
        "Let's examine what the LLM returns. This will be important when we integrate tools later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response Object Analysis\n",
            "==================================================\n",
            "\n",
            "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "\n",
            "Content: 2 + 2 = 4\n",
            "\n",
            "Response Metadata:\n",
            "   finish_reason: STOP\n",
            "   model_name: gemini-2.0-flash\n",
            "   safety_ratings: []\n",
            "   model_provider: google_genai\n"
          ]
        }
      ],
      "source": [
        "# Make another call to examine the response structure\n",
        "response = llm.invoke(\"What is 2 + 2?\")\n",
        "\n",
        "print(\"Response Object Analysis\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nType: {type(response)}\")\n",
        "print(f\"\\nContent: {response.content}\")\n",
        "print(f\"\\nResponse Metadata:\")\n",
        "for key, value in response.response_metadata.items():\n",
        "    print(f\"   {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='2 + 2 = 4', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c1c0a-b7a7-7ad2-9273-ee8c65de386c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference Point: AIMessage Structure\n",
        "\n",
        "The LLM returns an `AIMessage` object with these key attributes:\n",
        "\n",
        "| Attribute | Description |\n",
        "|-----------|-------------|\n",
        "| `.content` | The text response from the LLM |\n",
        "| `.response_metadata` | Model info, token usage, finish reason |\n",
        "| `.tool_calls` | List of tool calls (empty if no tools used) |\n",
        "| `.id` | Unique message identifier |\n",
        "\n",
        "> **Important for Tools:** When we add tools later, the LLM will populate `.tool_calls` instead of (or in addition to) `.content`. This is how the LLM requests tool execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 8: Optional - LangSmith Tracing\n",
        "\n",
        "LangSmith provides observability for your LLM applications. If you've configured it, let's verify it's working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith Status\n",
            "==================================================\n",
            "\n",
            "‚ö†Ô∏è  LangSmith tracing is DISABLED\n",
            "   To enable, add to your .env file:\n",
            "   LANGCHAIN_TRACING_V2=true\n",
            "   LANGSMITH_API_KEY=your-key\n"
          ]
        }
      ],
      "source": [
        "# Check LangSmith configuration\n",
        "langsmith_enabled = os.getenv(\"LANGCHAIN_TRACING_V2\") == \"true\"\n",
        "langsmith_project = os.getenv(\"LANGSMITH_PROJECT\", \"default\")\n",
        "\n",
        "print(\"LangSmith Status\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if langsmith_enabled:\n",
        "    print(f\"\\n‚úÖ LangSmith tracing is ENABLED\")\n",
        "    print(f\"   Project: {langsmith_project}\")\n",
        "    print(f\"\\n   View traces at: https://smith.langchain.com\")\n",
        "    print(f\"   Look for project: '{langsmith_project}'\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  LangSmith tracing is DISABLED\")\n",
        "    print(f\"   To enable, add to your .env file:\")\n",
        "    print(f\"   LANGCHAIN_TRACING_V2=true\")\n",
        "    print(f\"   LANGSMITH_API_KEY=your-key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference Point: Why Use LangSmith?\n",
        "\n",
        "LangSmith helps you:\n",
        "\n",
        "| Feature | Benefit |\n",
        "|---------|--------|\n",
        "| **Trace Visualization** | See the full execution flow of your agent |\n",
        "| **Token Usage** | Track costs across all LLM calls |\n",
        "| **Debugging** | Identify where errors occur in complex chains |\n",
        "| **Latency Analysis** | Find performance bottlenecks |\n",
        "\n",
        "> **Recommendation:** Enable LangSmith during development. It's invaluable for debugging tool-calling agents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this notebook, you completed:\n",
        "\n",
        "| Step | Status |\n",
        "|------|--------|\n",
        "| Installed dependencies | ‚úÖ |\n",
        "| Configured environment variables | ‚úÖ |\n",
        "| Imported required packages | ‚úÖ |\n",
        "| Initialized the LLM | ‚úÖ |\n",
        "| Verified connectivity | ‚úÖ |\n",
        "| Understood response structure | ‚úÖ |\n",
        "\n",
        "## Tutorial Series Overview\n",
        "\n",
        "You're now ready to proceed through the series:\n",
        "\n",
        "| Notebook | Topic | Description |\n",
        "|----------|-------|-------------|\n",
        "| **01** (this) | Setup & Validation | Environment configuration ‚úÖ |\n",
        "| **02** | Getting Started with Tools | `@tool` decorator, `.invoke()`, schemas |\n",
        "| **03** | Currency Converter | Multi-parameter tool with validation |\n",
        "| **04** | EMI Calculator | Complex calculations, mixed types |\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Proceed to **Notebook 02: Getting Started with LangGraph Tools** to learn how to create tools that extend LLM capabilities.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Reference: Reusable Code\n",
        "\n",
        "Copy this setup code to future notebooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STANDARD SETUP - Copy to new notebooks\n",
        "# ========================================\n",
        "\n",
        "# Imports\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools import tool\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment\n",
        "load_dotenv(\"../../.env\")  # Adjust path as needed\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Setup complete\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cbag-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
