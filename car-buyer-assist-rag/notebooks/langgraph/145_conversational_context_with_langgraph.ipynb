{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conversational Context with LangGraph\n",
        "\n",
        "Building multi-turn conversations where the agent maintains context across user interactions.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. **Manage conversational state** - Understand how MessagesState accumulates context across multiple user turns\n",
        "2. **Handle pronoun references** - Enable the agent to understand references like \"that RAV4\" or \"those two\" by maintaining conversation history\n",
        "3. **Build multi-turn workflows** - Create natural conversation flows for complex scenarios like car buying journeys\n",
        "4. **Track conversation growth** - Monitor how state expands with each interaction and understand message accumulation patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from typing import Literal, List\n",
        "\n",
        "load_dotenv(\"../../.env\")\n",
        "print(\"\u2705 Environment loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(\"\u2705 LLM initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Connect to Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChromaDB Configuration\n",
        "PERSIST_DIR = \"../../chroma_db\"\n",
        "COLLECTION_NAME = \"toyota_specs\"\n",
        "EMBED_MODEL_ID = \"gemini-embedding-001\"\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
        "    model=EMBED_MODEL_ID,\n",
        "    output_dimensionality=768\n",
        ")\n",
        "\n",
        "# Connect to vectorstore\n",
        "vectorstore = Chroma(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding_function=embeddings_model,\n",
        "    persist_directory=PERSIST_DIR\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Connected to vectorstore: {COLLECTION_NAME}\")\n",
        "print(f\"   Total documents: {vectorstore._collection.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector similarity search helper\n",
        "def vector_similarity_search(\n",
        "    query: str, \n",
        "    vectorstore, \n",
        "    k: int = 5\n",
        ") -> List[str]:\n",
        "    \"\"\"Perform vector similarity search.\"\"\"\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "    return [doc.page_content for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool 1: Vehicle Search\n",
        "@tool\n",
        "def search_vehicles(query: str, max_results: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Search Toyota vehicle database using semantic similarity.\n",
        "    \n",
        "    Use this tool when users need information about Toyota vehicles,\n",
        "    including specifications, pricing, fuel efficiency, or comparisons.\n",
        "    \n",
        "    Args:\n",
        "        query: Natural language search query about Toyota vehicles\n",
        "        max_results: Maximum number of results to return (default: 5)\n",
        "    \n",
        "    Returns:\n",
        "        Formatted string with vehicle information\n",
        "    \"\"\"\n",
        "    docs = vector_similarity_search(query, vectorstore, k=max_results)\n",
        "    \n",
        "    result = \"Vehicle Search Results:\\n\"\n",
        "    result += \"=\" * 60 + \"\\n\"\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        result += f\"\\nResult {i}:\\n{doc}\\n\"\n",
        "    result += \"=\" * 60\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"\u2705 search_vehicles tool defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool 2: EMI Calculator\n",
        "@tool\n",
        "def emi_calculator(principal: float, annual_interest_rate: float, tenure_months: int, currency: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculate the EMI (Equated Monthly Installment) for a loan.\n",
        "    \n",
        "    Use this tool when users want to know their monthly loan payment,\n",
        "    total repayment amount, or total interest for a loan.\n",
        "    \n",
        "    Args:\n",
        "        principal: The loan amount\n",
        "        annual_interest_rate: Annual interest rate as percentage (e.g., 8.5)\n",
        "        tenure_months: Loan tenure in months\n",
        "        currency: Currency code (USD, EUR, GBP, INR, JPY)\n",
        "    \"\"\"\n",
        "    if principal <= 0 or annual_interest_rate < 0 or tenure_months <= 0:\n",
        "        return \"Error: Invalid input parameters\"\n",
        "    \n",
        "    monthly_interest_rate = annual_interest_rate / 12 / 100\n",
        "    \n",
        "    if monthly_interest_rate == 0:\n",
        "        emi = principal / tenure_months\n",
        "        total_payment = principal\n",
        "        total_interest = 0\n",
        "    else:\n",
        "        emi = principal * monthly_interest_rate * \\\n",
        "              pow(1 + monthly_interest_rate, tenure_months) / \\\n",
        "              (pow(1 + monthly_interest_rate, tenure_months) - 1)\n",
        "        total_payment = emi * tenure_months\n",
        "        total_interest = total_payment - principal\n",
        "    \n",
        "    return (\n",
        "        f\"EMI Calculation Result:\\n\"\n",
        "        f\"  Loan Amount: {principal:,.2f} {currency}\\n\"\n",
        "        f\"  Interest Rate: {annual_interest_rate}% per annum\\n\"\n",
        "        f\"  Tenure: {tenure_months} months\\n\"\n",
        "        f\"  Monthly EMI: {emi:,.2f} {currency}\\n\"\n",
        "        f\"  Total Payment: {total_payment:,.2f} {currency}\\n\"\n",
        "        f\"  Total Interest: {total_interest:,.2f} {currency}\"\n",
        "    )\n",
        "\n",
        "print(\"\u2705 emi_calculator tool defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build LangGraph Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM with tools\n",
        "tools = [search_vehicles, emi_calculator]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def call_llm(state: MessagesState):\n",
        "    \"\"\"LLM node: Calls LLM with current messages.\"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
        "    \"\"\"Router: Check if agent wants to use tools.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# Build graph\n",
        "workflow = StateGraph(MessagesState)\n",
        "workflow.add_node(\"llm\", call_llm)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "workflow.add_edge(START, \"llm\")\n",
        "workflow.add_conditional_edges(\"llm\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "workflow.add_edge(\"tools\", \"llm\")\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"\u2705 Graph compiled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Understanding Conversational Context\n",
        "\n",
        "### What is Conversational Context?\n",
        "\n",
        "Conversational context is the ability to maintain information across multiple user turns in a conversation. This enables:\n",
        "\n",
        "**1. Pronoun Resolution**\n",
        "- User: \"Tell me about the RAV4\"\n",
        "- Agent: [provides RAV4 info]\n",
        "- User: \"What's the monthly payment for **it**?\" \u2190 Agent knows \"it\" = RAV4\n",
        "\n",
        "**2. Reference to Previous Results**\n",
        "- User: \"Compare Camry and RAV4 prices\"\n",
        "- Agent: [shows both prices]\n",
        "- User: \"Calculate EMI for **those two**\" \u2190 Agent knows which vehicles\n",
        "\n",
        "**3. Natural Follow-up Questions**\n",
        "- User: \"Show me SUVs\"\n",
        "- Agent: [lists SUVs]\n",
        "- User: \"Which one has the best mileage?\" \u2190 Agent knows we're still talking about SUVs\n",
        "\n",
        "### How MessagesState Enables This\n",
        "\n",
        "The `MessagesState` in LangGraph accumulates ALL messages from the conversation:\n",
        "- Every `HumanMessage` (user query)\n",
        "- Every `AIMessage` (agent response or tool call)\n",
        "- Every `ToolMessage` (tool result)\n",
        "\n",
        "This growing list of messages is passed to the LLM on each turn, giving it full conversation history.\n",
        "\n",
        "### Key Pattern for Multi-Turn Conversations\n",
        "\n",
        "```python\n",
        "# Initialize conversation state\n",
        "conversation = {\"messages\": []}\n",
        "\n",
        "# Turn 1\n",
        "conversation[\"messages\"].append(HumanMessage(content=\"First query\"))\n",
        "conversation = app.invoke(conversation)  # State now has 3-4 messages\n",
        "\n",
        "# Turn 2 - State persists!\n",
        "conversation[\"messages\"].append(HumanMessage(content=\"Follow-up query\"))\n",
        "conversation = app.invoke(conversation)  # State now has 6-8 messages\n",
        "```\n",
        "\n",
        "The state **accumulates** - it doesn't reset between turns!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Multi-Turn Car Buying Conversation\n",
        "\n",
        "Let's simulate a realistic car buying journey with multiple turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize conversation\n",
        "conversation = {\"messages\": []}\n",
        "\n",
        "print(\"MULTI-TURN CONVERSATIONAL CONTEXT TEST\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Scenario: Car buying journey with context maintained across turns\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Turn 1: Initial Vehicle Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn 1: Find vehicles\n",
        "print(\"TURN 1\")\n",
        "print(\"\u2500\" * 80)\n",
        "\n",
        "user_query_1 = \"I'm looking for an affordable Toyota SUV with good fuel economy. What do you recommend?\"\n",
        "print(f\"\ud83d\udc64 User: {user_query_1}\")\n",
        "\n",
        "conversation[\"messages\"].append(HumanMessage(content=user_query_1))\n",
        "conversation = app.invoke(conversation)\n",
        "\n",
        "print(f\"\\n\ud83e\udd16 Assistant: {conversation['messages'][-1].content}\")\n",
        "print(f\"\\n\ud83d\udcca State size: {len(conversation['messages'])} messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Turn 2: EMI Calculation for Mentioned Vehicle\n",
        "\n",
        "Notice the pronoun reference: \"**that RAV4**\" - the agent must remember the RAV4 from Turn 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn 2: Calculate EMI for recommended vehicle\n",
        "print(\"\\n\" + \"\u2500\" * 80)\n",
        "print(\"TURN 2\")\n",
        "print(\"\u2500\" * 80)\n",
        "\n",
        "user_query_2 = \"What would the monthly payment be for that RAV4 at 6.5% interest over 5 years?\"\n",
        "print(f\"\ud83d\udc64 User: {user_query_2}\")\n",
        "print(f\"\\n   \ud83d\udca1 Note: 'that RAV4' references Turn 1's recommendation!\\n\")\n",
        "\n",
        "conversation[\"messages\"].append(HumanMessage(content=user_query_2))\n",
        "conversation = app.invoke(conversation)\n",
        "\n",
        "print(f\"\ud83e\udd16 Assistant: {conversation['messages'][-1].content}\")\n",
        "print(f\"\\n\ud83d\udcca State size: {len(conversation['messages'])} messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Turn 3: Comparison Request\n",
        "\n",
        "The user asks for a comparison, establishing new context for the next turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn 3: Compare vehicles\n",
        "print(\"\\n\" + \"\u2500\" * 80)\n",
        "print(\"TURN 3\")\n",
        "print(\"\u2500\" * 80)\n",
        "\n",
        "user_query_3 = \"Can you compare the base prices of the RAV4 and Highlander for me?\"\n",
        "print(f\"\ud83d\udc64 User: {user_query_3}\\n\")\n",
        "\n",
        "conversation[\"messages\"].append(HumanMessage(content=user_query_3))\n",
        "conversation = app.invoke(conversation)\n",
        "\n",
        "print(f\"\ud83e\udd16 Assistant: {conversation['messages'][-1].content}\")\n",
        "print(f\"\\n\ud83d\udcca State size: {len(conversation['messages'])} messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Turn 4: Reference to Previous Comparison\n",
        "\n",
        "The phrase \"**those two**\" references the RAV4 and Highlander from Turn 3. The agent must maintain context to understand this reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn 4: Calculate for compared vehicles\n",
        "print(\"\\n\" + \"\u2500\" * 80)\n",
        "print(\"TURN 4\")\n",
        "print(\"\u2500\" * 80)\n",
        "\n",
        "user_query_4 = \"What would be the monthly payment difference between those two at 6% for 60 months?\"\n",
        "print(f\"\ud83d\udc64 User: {user_query_4}\")\n",
        "print(f\"\\n   \ud83d\udca1 Note: 'those two' references RAV4 & Highlander from Turn 3!\\n\")\n",
        "\n",
        "conversation[\"messages\"].append(HumanMessage(content=user_query_4))\n",
        "conversation = app.invoke(conversation)\n",
        "\n",
        "print(f\"\ud83e\udd16 Assistant: {conversation['messages'][-1].content}\")\n",
        "print(f\"\\n\ud83d\udcca State size: {len(conversation['messages'])} messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Analyze Complete Conversation State\n",
        "\n",
        "Let's examine the full conversation state to see how context accumulates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine conversation state\n",
        "print(\"COMPLETE CONVERSATION STATE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "turn_num = 0\n",
        "for i, msg in enumerate(conversation[\"messages\"], 1):\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        turn_num += 1\n",
        "        print(f\"\\n{'\u2500' * 80}\")\n",
        "        print(f\"TURN {turn_num}\")\n",
        "        print(f\"{'\u2500' * 80}\")\n",
        "        preview = msg.content[:60] + \"...\" if len(msg.content) > 60 else msg.content\n",
        "        print(f\"[{i}] \ud83d\udc64 USER: {preview}\")\n",
        "        \n",
        "    elif isinstance(msg, AIMessage):\n",
        "        if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "            tools_called = \", \".join([tc['name'] for tc in msg.tool_calls])\n",
        "            print(f\"[{i}] \ud83e\udd16 AGENT: Calling {tools_called}\")\n",
        "        else:\n",
        "            preview = msg.content[:50] + \"...\" if len(msg.content) > 50 else msg.content\n",
        "            print(f\"[{i}] \ud83e\udd16 AGENT: {preview}\")\n",
        "            \n",
        "    elif isinstance(msg, ToolMessage):\n",
        "        first_line = msg.content.split('\\n')[0]\n",
        "        print(f\"[{i}] \ud83d\udd27 TOOL: {first_line}\")\n",
        "\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(f\"Total messages in state: {len(conversation['messages'])}\")\n",
        "\n",
        "# Count message types\n",
        "human_count = sum(1 for m in conversation[\"messages\"] if isinstance(m, HumanMessage))\n",
        "ai_count = sum(1 for m in conversation[\"messages\"] if isinstance(m, AIMessage))\n",
        "tool_count = sum(1 for m in conversation[\"messages\"] if isinstance(m, ToolMessage))\n",
        "\n",
        "print(f\"\\nMessage breakdown:\")\n",
        "print(f\"  \ud83d\udc64 HumanMessages:  {human_count}\")\n",
        "print(f\"  \ud83e\udd16 AIMessages:     {ai_count}\")\n",
        "print(f\"  \ud83d\udd27 ToolMessages:   {tool_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize State Growth Over Time\n",
        "\n",
        "Track how the conversation state grows with each turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show state growth\n",
        "print(\"STATE GROWTH ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nTurn | Messages Added | Total Messages | Context Window\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Simulate tracking (approximation based on typical patterns)\n",
        "turn_data = [\n",
        "    (\"Start\", 0, 0, \"Empty\"),\n",
        "    (\"Turn 1\", 4, 4, \"Initial vehicle search\"),\n",
        "    (\"Turn 2\", 2, 6, \"+ EMI calculation request\"),\n",
        "    (\"Turn 3\", 4, 10, \"+ Vehicle comparison\"),\n",
        "    (\"Turn 4\", 5, 15, \"+ Payment difference calculation\")\n",
        "]\n",
        "\n",
        "for turn, added, total, context in turn_data:\n",
        "    print(f\"{turn:6s} | {added:14d} | {total:14d} | {context}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Key Insight: Each turn adds 2-5 messages (query + response + tool calls/results)\")\n",
        "print(\"The LLM receives ALL previous messages on each turn, enabling context awareness.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Context Understanding\n",
        "\n",
        "Let's verify the agent truly understands context with a challenging follow-up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test context recall\n",
        "print(\"CONTEXT RECALL TEST\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "user_query_5 = \"Remind me, what was the monthly payment difference between the two SUVs we discussed?\"\n",
        "print(f\"\ud83d\udc64 User: {user_query_5}\")\n",
        "print(f\"\\n   \ud83d\udca1 Testing if agent remembers: RAV4 vs Highlander comparison from Turn 4\\n\")\n",
        "\n",
        "conversation[\"messages\"].append(HumanMessage(content=user_query_5))\n",
        "conversation = app.invoke(conversation)\n",
        "\n",
        "print(f\"\ud83e\udd16 Assistant: {conversation['messages'][-1].content}\")\n",
        "print(f\"\\n\ud83d\udcca Final state size: {len(conversation['messages'])} messages\")\n",
        "print(\"\\n\u2705 If the agent correctly recalls the payment difference, context is working!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Inspect Message Details by Turn\n",
        "\n",
        "Examine specific messages to see how context is maintained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show first few messages\n",
        "print(\"FIRST 5 MESSAGES (Turn 1)\")\n",
        "print(\"=\" * 80)\n",
        "for i in range(min(5, len(conversation['messages']))):\n",
        "    msg = conversation['messages'][i]\n",
        "    msg_type = type(msg).__name__\n",
        "    print(f\"\\n[{i}] {msg_type}:\")\n",
        "    if isinstance(msg, (HumanMessage, AIMessage)):\n",
        "        preview = msg.content[:100] if msg.content else \"[tool calls]\"\n",
        "        print(f\"  {preview}...\")\n",
        "    elif isinstance(msg, ToolMessage):\n",
        "        print(f\"  {msg.content[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all messages\n",
        "conversation[\"messages\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Understanding Context Windows\n",
        "\n",
        "### What Gets Passed to the LLM?\n",
        "\n",
        "On each turn, the **entire** conversation history is sent to the LLM:\n",
        "\n",
        "```python\n",
        "# Turn 1\n",
        "LLM receives: [HumanMessage\u2081]\n",
        "\n",
        "# Turn 2  \n",
        "LLM receives: [HumanMessage\u2081, AIMessage\u2081, ToolMessage\u2081, AIMessage\u2081_final, HumanMessage\u2082]\n",
        "\n",
        "# Turn 3\n",
        "LLM receives: [all messages from Turns 1-2, HumanMessage\u2083]\n",
        "```\n",
        "\n",
        "This is why the agent can understand:\n",
        "- \"that RAV4\" (mentioned in Turn 1)\n",
        "- \"those two\" (RAV4 and Highlander compared in Turn 3)\n",
        "- \"the two SUVs we discussed\" (Turn 5 recalls Turn 4)\n",
        "\n",
        "### Practical Considerations\n",
        "\n",
        "**Advantages:**\n",
        "- Full context awareness\n",
        "- Natural conversation flow\n",
        "- No need to repeat information\n",
        "\n",
        "**Challenges:**\n",
        "- Context window limits (models have max token limits)\n",
        "- Increased latency (more tokens to process)\n",
        "- Cost considerations (more tokens = higher API costs)\n",
        "\n",
        "**Best Practices:**\n",
        "- Summarize old messages after many turns\n",
        "- Implement context pruning strategies\n",
        "- Monitor state size in production applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Pattern Comparison\n",
        "\n",
        "Compare conversational context with other execution patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare execution patterns\n",
        "print(\"EXECUTION PATTERN COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n1. PARALLEL EXECUTION:\")\n",
        "print(\"   Pattern: Single AIMessage with multiple tool_calls\")\n",
        "print(\"   Message count: ~5\")\n",
        "print(\"   Use case: Independent tasks (search + calculate)\")\n",
        "print(\"   Turns: 1\")\n",
        "\n",
        "print(\"\\n2. SEQUENTIAL EXECUTION:\")\n",
        "print(\"   Pattern: Multiple separate AIMessages with tool_calls\")\n",
        "print(\"   Message count: ~6\")\n",
        "print(\"   Use case: Dependent tasks (search result \u2192 EMI input)\")\n",
        "print(\"   Turns: 1\")\n",
        "\n",
        "print(\"\\n3. CONVERSATIONAL CONTEXT:\")\n",
        "print(\"   Pattern: State accumulates across multiple user turns\")\n",
        "print(f\"   Message count: {len(conversation['messages'])} (in our example)\")\n",
        "print(\"   Use case: Multi-turn car buying journey\")\n",
        "print(f\"   Turns: {human_count}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Key Difference: Conversational context spans MULTIPLE user queries,\")\n",
        "print(\"while parallel/sequential patterns complete within a SINGLE query.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Additional Conversation Example\n",
        "\n",
        "Start a fresh conversation to demonstrate another use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start a new conversation\n",
        "conversation2 = {\"messages\": []}\n",
        "\n",
        "print(\"NEW CONVERSATION: Budget-Focused Buyer\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Turn 1\n",
        "query1 = \"I have a budget of $400/month. What Toyota vehicles can I afford?\"\n",
        "print(f\"\\nTurn 1: {query1}\")\n",
        "conversation2[\"messages\"].append(HumanMessage(content=query1))\n",
        "conversation2 = app.invoke(conversation2)\n",
        "print(f\"Response: {conversation2['messages'][-1].content[:150]}...\")\n",
        "print(f\"State: {len(conversation2['messages'])} messages\")\n",
        "\n",
        "# Turn 2\n",
        "query2 = \"Which of those has the best fuel economy?\"\n",
        "print(f\"\\nTurn 2: {query2}\")\n",
        "print(\"   \ud83d\udca1 'those' refers to vehicles mentioned in Turn 1\")\n",
        "conversation2[\"messages\"].append(HumanMessage(content=query2))\n",
        "conversation2 = app.invoke(conversation2)\n",
        "print(f\"Response: {conversation2['messages'][-1].content[:150]}...\")\n",
        "print(f\"State: {len(conversation2['messages'])} messages\")\n",
        "\n",
        "# Turn 3\n",
        "query3 = \"Tell me more about its safety features.\"\n",
        "print(f\"\\nTurn 3: {query3}\")\n",
        "print(\"   \ud83d\udca1 'its' refers to the vehicle selected in Turn 2\")\n",
        "conversation2[\"messages\"].append(HumanMessage(content=query3))\n",
        "conversation2 = app.invoke(conversation2)\n",
        "print(f\"Response: {conversation2['messages'][-1].content[:150]}...\")\n",
        "print(f\"State: {len(conversation2['messages'])} messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "\u2705 **Conversational state management** - MessagesState accumulates ALL messages (HumanMessage, AIMessage, ToolMessage) across turns, enabling the agent to maintain full conversation context\n",
        "\n",
        "\u2705 **Pronoun and reference resolution** - The agent successfully resolves references like \"that RAV4\" (Turn 2 \u2192 Turn 1), \"those two\" (Turn 4 \u2192 Turn 3), and \"the two SUVs we discussed\" (Turn 5 \u2192 Turn 4) by accessing conversation history\n",
        "\n",
        "\u2705 **Multi-turn workflow patterns** - Built natural conversation flows for complex scenarios (4-5 turns, 15+ messages) where each turn builds on previous interactions, demonstrating real-world car buying journeys\n",
        "\n",
        "\u2705 **State growth tracking** - Monitored how state expands from 0 \u2192 4 \u2192 6 \u2192 10 \u2192 15+ messages across turns, understanding that each turn adds 2-5 messages depending on tool usage\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "**Context Accumulation Pattern:**\n",
        "```\n",
        "Turn 1: [H\u2081, AI\u2081, T\u2081, AI\u2081_final] = 4 messages\n",
        "Turn 2: [Previous + H\u2082, AI\u2082_final] = 6 messages  \n",
        "Turn 3: [Previous + H\u2083, AI\u2083, T\u2083, AI\u2083_final] = 10 messages\n",
        "Turn 4: [Previous + H\u2084, AI\u2084, T\u2084, T\u2084, AI\u2084_final] = 15 messages\n",
        "```\n",
        "\n",
        "**Why This Matters:**\n",
        "- **Natural UX**: Users don't need to repeat context (\"the RAV4 you mentioned\" vs just \"it\")\n",
        "- **Intelligent Assistance**: Agent understands implicit references and maintains conversation thread\n",
        "- **Real Conversations**: Enables multi-step processes like car buying, travel planning, or complex research\n",
        "\n",
        "**Practical Considerations:**\n",
        "- **Token Limits**: Most LLMs have context window limits (e.g., 32K, 128K tokens)\n",
        "- **Cost**: More messages = more tokens processed = higher API costs\n",
        "- **Latency**: Larger context windows take longer to process\n",
        "- **Production Strategies**: Implement summarization, pruning, or context compression for long conversations\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "This conversational pattern is essential for:\n",
        "- **Customer Support**: Multi-turn troubleshooting with context\n",
        "- **E-commerce**: Product search \u2192 comparison \u2192 purchase decision flow\n",
        "- **Healthcare**: Symptom discussion \u2192 diagnosis \u2192 treatment planning\n",
        "- **Education**: Tutoring sessions with progressive concept building\n",
        "- **Financial Planning**: Budget analysis \u2192 investment options \u2192 recommendations\n",
        "\n",
        "### Series Complete!\n",
        "\n",
        "Congratulations! You've mastered the three core execution patterns in LangGraph:\n",
        "\n",
        "1. **Parallel Execution** - Independent tasks executed simultaneously (5 messages)\n",
        "2. **Sequential Execution** - Dependent tasks executed in order (6+ messages)  \n",
        "3. **Conversational Context** - Multi-turn conversations with state accumulation (15+ messages)\n",
        "\n",
        "Combined with vector similarity search and computational tools, you now have the complete toolkit to build sophisticated, production-ready agentic workflows with LangGraph!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cbag-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}