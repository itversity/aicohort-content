{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph Graph Compilation & Testing\n",
        "\n",
        "Compile, invoke, and test your LangGraph workflow with different scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. **Understand compilation** \u2014 What `compile()` does and why it's required\n",
        "2. **Invoke the graph** \u2014 Run queries and examine the results\n",
        "3. **Test both execution paths** \u2014 Tool path (LLM \u2192 Tool \u2192 LLM) vs direct path (LLM \u2192 END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup & Graph Build\n",
        "\n",
        "We rebuild the same graph from the previous notebook. The setup is compact since we've already learned each component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\"../../.env\")\n",
        "print(\"\u2705 Environment loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from typing import Literal\n",
        "\n",
        "print(\"\u2705 All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define tools\n",
        "@tool\n",
        "def currency_converter(amount: float, from_currency: str, to_currency: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert currency from one type to another.\n",
        "    \n",
        "    Use this tool when users need to convert monetary amounts between\n",
        "    different currencies. Supports USD, EUR, GBP, INR, and JPY.\n",
        "    \"\"\"\n",
        "    exchange_rates = {\"USD\": 1.0, \"EUR\": 0.92, \"GBP\": 0.79, \"INR\": 83.12, \"JPY\": 149.50}\n",
        "    \n",
        "    from_currency = from_currency.upper()\n",
        "    to_currency = to_currency.upper()\n",
        "    \n",
        "    if from_currency not in exchange_rates:\n",
        "        return f\"Error: Unsupported currency {from_currency}\"\n",
        "    if to_currency not in exchange_rates:\n",
        "        return f\"Error: Unsupported currency {to_currency}\"\n",
        "    \n",
        "    amount_in_usd = amount / exchange_rates[from_currency]\n",
        "    converted_amount = amount_in_usd * exchange_rates[to_currency]\n",
        "    effective_rate = exchange_rates[to_currency] / exchange_rates[from_currency]\n",
        "    \n",
        "    return (\n",
        "        f\"Conversion Result:\\n\"\n",
        "        f\"  {amount:,.2f} {from_currency} = {converted_amount:,.2f} {to_currency}\\n\"\n",
        "        f\"  Exchange Rate: 1 {from_currency} = {effective_rate:.4f} {to_currency}\"\n",
        "    )\n",
        "\n",
        "@tool\n",
        "def emi_calculator(principal: float, annual_interest_rate: float, tenure_months: int, currency: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculate the EMI (Equated Monthly Installment) for a loan.\n",
        "    \n",
        "    Use this tool when users want to know their monthly loan payment,\n",
        "    total repayment amount, or total interest for a loan.\n",
        "    \"\"\"\n",
        "    if principal <= 0:\n",
        "        return \"Error: Principal must be greater than 0\"\n",
        "    if annual_interest_rate < 0:\n",
        "        return \"Error: Interest rate cannot be negative\"\n",
        "    if tenure_months <= 0:\n",
        "        return \"Error: Tenure must be greater than 0\"\n",
        "    \n",
        "    monthly_interest_rate = annual_interest_rate / 12 / 100\n",
        "    \n",
        "    if monthly_interest_rate == 0:\n",
        "        emi = principal / tenure_months\n",
        "        total_payment = principal\n",
        "        total_interest = 0\n",
        "    else:\n",
        "        emi = principal * monthly_interest_rate * \\\n",
        "              pow(1 + monthly_interest_rate, tenure_months) / \\\n",
        "              (pow(1 + monthly_interest_rate, tenure_months) - 1)\n",
        "        total_payment = emi * tenure_months\n",
        "        total_interest = total_payment - principal\n",
        "    \n",
        "    return (\n",
        "        f\"EMI Calculation Result:\\n\"\n",
        "        f\"  Loan Amount: {principal:,.2f} {currency}\\n\"\n",
        "        f\"  Interest Rate: {annual_interest_rate}% per annum\\n\"\n",
        "        f\"  Tenure: {tenure_months} months\\n\"\n",
        "        f\"  Monthly EMI: {emi:,.2f} {currency}\\n\"\n",
        "        f\"  Total Payment: {total_payment:,.2f} {currency}\\n\"\n",
        "        f\"  Total Interest: {total_interest:,.2f} {currency}\"\n",
        "    )\n",
        "\n",
        "print(\"\u2705 Tools defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM and bind tools\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "tools = [currency_converter, emi_calculator]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"\u2705 LLM initialized with tools\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build graph (same as previous notebook)\n",
        "def call_llm(state: MessagesState):\n",
        "    \"\"\"LLM node that invokes the LLM.\"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
        "    \"\"\"Router that decides next step based on tool_calls.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "workflow = StateGraph(MessagesState)\n",
        "workflow.add_node(\"llm\", call_llm)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "workflow.add_edge(START, \"llm\")\n",
        "workflow.add_conditional_edges(\"llm\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "workflow.add_edge(\"tools\", \"llm\")\n",
        "\n",
        "print(\"\u2705 Graph built\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding Compilation\n",
        "\n",
        "Before we can use the graph, we must **compile** it. Compilation does two things:\n",
        "\n",
        "1. **Validates the graph** \u2014 Checks that all nodes are reachable from START and all edges point to valid nodes\n",
        "2. **Creates an executable** \u2014 Returns a `CompiledGraph` object with `invoke()` and `stream()` methods\n",
        "\n",
        "Think of it like compiling code: you define the graph (write the source), then compile it (create the executable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "app = workflow.compile()\n",
        "\n",
        "print(f\"Type: {type(app).__name__}\")\n",
        "print(\"\u2705 Graph compiled successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What happens if compilation fails?\n",
        "\n",
        "Let's intentionally create an invalid graph to see the error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: A graph with a node that has no incoming edge\n",
        "bad_workflow = StateGraph(MessagesState)\n",
        "bad_workflow.add_node(\"llm\", call_llm)\n",
        "bad_workflow.add_node(\"orphan\", call_llm)  # No edge leads to this node\n",
        "bad_workflow.add_edge(START, \"llm\")\n",
        "bad_workflow.add_edge(\"llm\", END)\n",
        "\n",
        "try:\n",
        "    bad_workflow.compile()\n",
        "    print(\"Compiled (orphan nodes may be allowed but ignored)\")\n",
        "except Exception as e:\n",
        "    print(f\"Compilation error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Invoking the Graph\n",
        "\n",
        "The `invoke()` method takes an initial state and runs it through the graph until it reaches END.\n",
        "\n",
        "```python\n",
        "result = app.invoke({\"messages\": [HumanMessage(content=\"...\")]})\n",
        "```\n",
        "\n",
        "The result is the **final state** containing all messages generated during execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test: Tool Execution Path\n",
        "\n",
        "When the query requires a tool, the graph follows: **START \u2192 llm \u2192 tools \u2192 llm \u2192 END**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Currency conversion (requires tool)\n",
        "result1 = app.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"Convert 500 USD to INR\")]\n",
        "})\n",
        "\n",
        "print(\"TEST 1: Currency Conversion\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Messages generated: {len(result1['messages'])}\")\n",
        "print(f\"Path: START \u2192 llm \u2192 tools \u2192 llm \u2192 END\")\n",
        "print(f\"\\nFinal Response:\")\n",
        "print(result1['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: EMI calculation (requires tool)\n",
        "result2 = app.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"Calculate EMI for a 2,000,000 INR home loan at 8.5% for 20 years\")]\n",
        "})\n",
        "\n",
        "print(\"TEST 2: EMI Calculation\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Messages generated: {len(result2['messages'])}\")\n",
        "print(f\"Path: START \u2192 llm \u2192 tools \u2192 llm \u2192 END\")\n",
        "print(f\"\\nFinal Response:\")\n",
        "print(result2['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both tool queries generate **4 messages**:\n",
        "\n",
        "| # | Message Type | Created By |\n",
        "|---|---|---|\n",
        "| 1 | `HumanMessage` | User (our input) |\n",
        "| 2 | `AIMessage` with `tool_calls` | LLM node (first pass) |\n",
        "| 3 | `ToolMessage` | Tool node |\n",
        "| 4 | `AIMessage` with `content` | LLM node (second pass) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test: Direct Path (No Tool)\n",
        "\n",
        "When the query doesn't need a tool, the graph follows: **START \u2192 llm \u2192 END**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: General greeting (no tool needed)\n",
        "result3 = app.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"Hello! What can you help me with?\")]\n",
        "})\n",
        "\n",
        "print(\"TEST 3: General Greeting\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Messages generated: {len(result3['messages'])}\")\n",
        "print(f\"Path: START \u2192 llm \u2192 END\")\n",
        "print(f\"\\nFinal Response:\")\n",
        "print(result3['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4: General knowledge question (no tool needed)\n",
        "result4 = app.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What does EMI stand for?\")]\n",
        "})\n",
        "\n",
        "print(\"TEST 4: General Knowledge\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Messages generated: {len(result4['messages'])}\")\n",
        "print(f\"Path: START \u2192 llm \u2192 END\")\n",
        "print(f\"\\nFinal Response:\")\n",
        "print(result4['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Non-tool queries generate only **2 messages**:\n",
        "\n",
        "| # | Message Type | Created By |\n",
        "|---|---|---|\n",
        "| 1 | `HumanMessage` | User (our input) |\n",
        "| 2 | `AIMessage` with `content` | LLM node (responds directly) |\n",
        "\n",
        "The router sees no `tool_calls` in the AIMessage, so it routes directly to END."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparing Both Paths\n",
        "\n",
        "Let's summarize the message counts across all tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary comparison\n",
        "tests = [\n",
        "    (\"Convert 500 USD to INR\", result1),\n",
        "    (\"Calculate EMI for home loan\", result2),\n",
        "    (\"Hello! What can you help me with?\", result3),\n",
        "    (\"What does EMI stand for?\", result4)\n",
        "]\n",
        "\n",
        "print(\"EXECUTION PATH COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Query':<40} {'Messages':>10} {'Path':>18}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for query, result in tests:\n",
        "    msg_count = len(result['messages'])\n",
        "    # Check if any AIMessage has tool_calls\n",
        "    has_tool = any(\n",
        "        hasattr(m, 'tool_calls') and m.tool_calls \n",
        "        for m in result['messages'] if isinstance(m, AIMessage)\n",
        "    )\n",
        "    path = \"llm \u2192 tools \u2192 llm\" if has_tool else \"llm \u2192 END\"\n",
        "    query_short = query[:38] + \"..\" if len(query) > 40 else query\n",
        "    print(f\"{query_short:<40} {msg_count:>10} {path:>18}\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Understanding the Result State\n",
        "\n",
        "The result from `invoke()` is a dictionary with the same structure as `MessagesState`. Let's inspect it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the result structure\n",
        "print(f\"Result type: {type(result1)}\")\n",
        "print(f\"Result keys: {list(result1.keys())}\")\n",
        "print(f\"Messages type: {type(result1['messages'])}\")\n",
        "print(f\"Messages count: {len(result1['messages'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect each message's type in the tool execution result\n",
        "print(\"Message types in tool execution result:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, msg in enumerate(result1['messages']):\n",
        "    msg_type = type(msg).__name__\n",
        "    \n",
        "    if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "        msg_type += \" (with tool_calls)\"\n",
        "    elif isinstance(msg, AIMessage):\n",
        "        msg_type += \" (final response)\"\n",
        "    \n",
        "    print(f\"  [{i}] {msg_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect each message's type in the direct path result\n",
        "print(\"Message types in direct path result:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, msg in enumerate(result3['messages']):\n",
        "    msg_type = type(msg).__name__\n",
        "    \n",
        "    if isinstance(msg, AIMessage):\n",
        "        msg_type += \" (final response)\"\n",
        "    \n",
        "    print(f\"  [{i}] {msg_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "### What You've Accomplished\n",
        "\n",
        "\u2705 **Understood compilation** \u2014 `compile()` validates the graph and creates an executable application\n",
        "\n",
        "\u2705 **Invoked the graph** \u2014 Used `invoke()` to run queries and examined the result state\n",
        "\n",
        "\u2705 **Tested both execution paths:**\n",
        "- **Tool path** (4 messages): HumanMessage \u2192 AIMessage with tool_calls \u2192 ToolMessage \u2192 AIMessage with content\n",
        "- **Direct path** (2 messages): HumanMessage \u2192 AIMessage with content\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "The **router function** is the decision point. It checks whether the LLM's response contains `tool_calls`. If yes, the graph loops through the tool node and back to the LLM. If no, the graph ends. This simple check enables the LLM to dynamically decide whether to use tools.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Continue to **Notebook 075: Single Tool Execution** to deep-dive into the message flow and examine the internal structure of each message type."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cbag-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbformat_minor": 4,
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
